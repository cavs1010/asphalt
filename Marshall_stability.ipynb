{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('spyder': conda)"
  },
  "interpreter": {
   "hash": "fc22dc884bcedff190aa0e4687df8895271c55103986e14a4799605b50a1eed1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Marshall Stability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% IMPORTS\n",
    "#BASICS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import absolute\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import display, Markdown, Latex\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#STATISTICS\n",
    "from scipy.stats import normaltest\n",
    "from scipy import stats\n",
    "\n",
    "#ML TRAINING AND DATA PREPROCESSING\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#ML MODELS\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "#MODEL EVALUATION\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#METRICS\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "source": [
    "## 1. Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate Outliers based on the interquantile\n",
    "#datFrame: Data frame where the outliers will be eliminated.\n",
    "#columnName: the name of the column where the outliers will be identified.\n",
    "def eliminateOutliers (dataFrame, columnName):\n",
    "    Q1 = dataFrame[columnName].quantile(0.25)\n",
    "    Q3 = dataFrame[columnName].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    print('Initial dataframe size: '+str(dataFrame.shape))\n",
    "    dataFrame = dataFrame[(dataFrame[columnName] < (Q3 + 1.5 * IQR)) & (dataFrame[columnName] > (Q1 - 1.5 * IQR))]\n",
    "    print('Final dataframe size: '+str(dataFrame.shape))\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot graphs for the categorical variables\n",
    "# dataFrame: Data frame associated to the property of interest (dfAirVoids, dfMS, dfMF, dfITS, dfTSR)\n",
    "# propertyOfInterest: the name of the column where the property of interest is located.\n",
    "# columnName1...4: The categorical columns to evaluate.\n",
    "def displayBoxPlotGraphs (dataFrame, propertyOfInterest, columnName1, columnName2, columnName3, columnName4):\n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15,10))\n",
    "    sns.boxplot(y = propertyOfInterest, x = columnName1, data=dataFrame,  orient='v' , ax=ax1)\n",
    "    sns.boxplot(y = propertyOfInterest, x = columnName2, data=dataFrame,  orient='v' , ax=ax2)\n",
    "    sns.boxplot(y = propertyOfInterest, x= columnName3, data=dataFrame,  orient='v' , ax=ax3)\n",
    "    sns.boxplot(y= propertyOfInterest, x= columnName4, data=dataFrame,  orient='v' , ax=ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that print the best parameters, R2 and MSE based on a grid search.\n",
    "def printBestModel (grid):\n",
    "    mse = -grid.best_score_\n",
    "    print('Best Parameters:' , grid.best_params_)\n",
    "    print('Best MSE:' + str(mse))"
   ]
  },
  {
   "source": [
    "## 2. Data Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%DATA READING AND INITIAL PREPROCESSING\n",
    "numericColumns = ['Aggregate absorption (%)',\n",
    "                  'Apparent specific gravity',\n",
    "                    0.075,\n",
    "                    0.3,\n",
    "                    0.6,\n",
    "                    2.36,\n",
    "                    4.75,\n",
    "                    9.5,\n",
    "                    12.5,\n",
    "                    19,\n",
    "                    'Plastic particle size (mm)',\n",
    "                    'Mixing speed (RPM)',\n",
    "                    'Mixing Temperature',\n",
    "                    'Mixing Time (hours)',\n",
    "                    'Plastic Addition by bitumen weight (%)',\n",
    "                    'Bitumen content in the sample'\n",
    "                    ]\n",
    "categoricalColumns = ['Modified asphalt Mix?',\n",
    "                      'Agreggate Type',\n",
    "                    'Aggregate absorption [%]',\n",
    "                    'Filler used',\n",
    "                    'Consolidated bitumen penetration grade',\n",
    "                    'New Plastic Type',\n",
    "                    'Plastic pretreatment',\n",
    "                    'Plastic shape',\n",
    "                    'Plastic Size',\n",
    "                    'Mixing Process',\n",
    "                    'Plastic melted previous to addition?',\n",
    "                    'Aggregates replacement ?',\n",
    "                    'Bitumen replacement?',\n",
    "                    'Filler replacement',\n",
    "                    'Property',\n",
    "                    'Units']\n",
    "#It returns the dataframe of interes based on the property - 'AirVoids', 'MS', 'MF', 'ITS', 'TSR'\n",
    "def returnDf (propertyOfInterest):\n",
    "    df = pd.read_excel('fileML.xlsx', sheet_name = propertyOfInterest, engine='openpyxl')\n",
    "    df = df.set_index(propertyOfInterest + ' ID')\n",
    "    df.loc[:,:'Units'] = df.loc[:,:'Units'].applymap(str)\n",
    "    df.loc[:,:'Units'] = df.loc[:,:'Units'] .applymap(str.strip)\n",
    "    df.replace('NS', np.nan, inplace = True)\n",
    "    df[numericColumns] = df[numericColumns].replace('N/a', 0).astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS = returnDf('MS')"
   ]
  },
  {
   "source": [
    "## 3. Data Exploration\n",
    "###  3.1 Total Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS = eliminateOutliers(dfMS, 'MS of the sample (kN)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS.iloc[:,2:].describe(include = 'all')"
   ]
  },
  {
   "source": [
    "I might have a problem with the $\\color{red}{\\text{Aggregate absorption}}$ because more than 20% of the data is missing. Regarding the $\\color{red}{\\text{MS}}$, there is a high dispersion ($\\sigma$ = 4.56), and the Mean seems normal. According to the Australian standards, the minimum value of the Marshall stability is between two and eigth."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfMS[['Aggregate absorption (%)', 'Apparent specific gravity', 'Bitumen content in the sample', 'MS of the sample (kN)']], figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfMS.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap MS', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "source": [
    "Interestingly, there is positive correlation in $\\color{red}{\\text{MS-Apparent specific gravity}}$ and $\\color{red}{\\text{MS-plastic addition by bitumen content}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfMS, propertyOfInterest = \"MS of the sample (kN)\", columnName1 = \"Agreggate Type\", columnName2 = \"Filler used\", columnName3 = \"Consolidated bitumen penetration grade\", columnName4 = \"Modified asphalt Mix?\")"
   ]
  },
  {
   "source": [
    "*   As it happened with the Air Voids, it exists a MS difference among the samples that employed the bitumen 40/50; however, it is important to note that the sample size for this group was not representative enough.\n",
    "\n",
    "*   Samples with plastic modification tend to have higher MS. The glue effect of the plastic and the stiffness increase of the bitumen might serve as valid explanations.\n",
    "\n",
    "*   No signigicant difference among the aggregate types and fillers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###  3.2 Modified mixtures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSModvsUnmod = dfMS [['Modified asphalt Mix?', 'MS of the sample (kN)']]\n",
    "dfMSModvsUnmod.groupby(['Modified asphalt Mix?'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSModified = dfMS[dfMS['Modified asphalt Mix?'] == 'Yes']\n",
    "dfMSModified.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInteres = numericColumns[0:2]+numericColumns[10:]+['MS of the sample (kN)']\n",
    "scatter_matrix(dfMSModified[columnsOfInteres], figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfMSModified.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap MS', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "source": [
    " $\\color{red}{\\text{MS-Apparent specific gravity}}$ presents the highest correlation with  $\\color{red}{\\text{MS}}$; however, it only has 66 observations, so it is not a convincing result. Other parameters such as  $\\color{red}{\\text{Plastic content}}$, and  $\\color{red}{\\text{gradation}}$ present an slight effect on the MS."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfMSModified, propertyOfInterest = \"MS of the sample (kN)\", columnName1 = \"Agreggate Type\", columnName2 = \"Plastic shape\", columnName3 = \"New Plastic Type\", columnName4 = \"Mixing Process\")"
   ]
  },
  {
   "source": [
    "The mean of the **dry** and **wet** process are not significantly different."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###  3.3 Wet vs. Dry Mixing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSWetvsDry = dfMSModified [['Mixing Process', 'MS of the sample (kN)']]\n",
    "dfMSWetvsDry.groupby(['Mixing Process'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfMSModified[columnsOfInteres+['Mixing Process']], hue=\"Mixing Process\", height=2.5)"
   ]
  },
  {
   "source": [
    "##  **Marshall Stability summary:**\n",
    "\n",
    "*   There are missing values mainly in $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{Aggregate type}}$ and $\\color{red}{\\text{filler used}}$.\n",
    "*   Four outliers were eliminated. The final total sample included 402 data points ($\\mu$ = 14.47, $\\sigma$ = 4.6). \n",
    "*   $\\color{red}{\\text{Aggregate absorption}}$ seems to be a critical variable to include, but the percentage of missing values is more than 20%.\n",
    "*   $\\color{red}{\\text{Apparent specific gravity}}$ presents the strongest positive correlation with the Marshall stability, but it is not a reliable inference becasue it presents many missing points (318 missing points).\n",
    "* Although Marshall stability of modified asphalts is relatively higher than not modified, this is not certain because the high variances of both sample groups. $\\mu_{modified}$ = 15.12 vs. $\\mu_{unmodified}$ = 11.97\n",
    "*   $\\color{red}{\\text{Percentage of plastic addition}}$ has a noticeable possitive correlation with MS. (r = 0.39) \n",
    "*   MS of dry and wet are really similar -> $\\mu_{Dry}$ = 15.05 (200 observations) vs $\\mu_{Wet}$ = 15.2 (119 observations)\n",
    "\n",
    "## 4. Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS.info()"
   ]
  },
  {
   "source": [
    "###  Pre-processing:\n",
    "1.  Eliminate the columns $\\color{red}{\\text{Article ID}}$, $\\color{red}{\\text{Global ID}}$, $\\color{red}{\\text{Aggregate type}}$, $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{filler used}}$, $\\color{red}{\\text{Bitumen type penetration}}$, $\\color{red}{\\text{Property}}$, $\\color{red}{\\text{plastic size}}$ and $\\color{red}{\\text{Units}}$.\n",
    "2.  Change the N/a to zero. This is for the unmodified mixtures.\n",
    "3.  Eliminate rows with missing values in $\\color{red}{\\text{New Plastic Type}}$, $\\color{red}{\\text{Plastic addition by bitumen weight}}$ and $\\color{red}{\\text{bitumen}}$ content in sample\n",
    "4.  Change categorical columns to numeric.\n",
    "5.  Imputer to $\\color{red}{\\text{Aggregate absorption}}$, $\\color{red}{\\text{gradation}}$, $\\color{red}{\\text{plastic size(mm)}}$, and $\\color{red}{\\text{mixing parameters}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Variables\n",
    "dfMSCleaned = dfMS.drop(['Article ID', \n",
    "                        'Global ID',\n",
    "                        'Modified asphalt Mix?',\n",
    "                        'Agreggate Type', \n",
    "                        'Apparent specific gravity', \n",
    "                        'Filler used', \n",
    "                        'Bitumen Type Penetration Grade', \n",
    "                        'Property', \n",
    "                        'Units', \n",
    "                        'Plastic Size' ], axis = 1)\n",
    "dfMSCleaned = dfMSCleaned.replace('N/a', 0)\n",
    "dfMSCleaned = dfMSCleaned.dropna(subset=['New Plastic Type', \n",
    "                                        'Plastic Addition by bitumen weight (%)', \n",
    "                                        'Bitumen content in the sample'])\n",
    "dfMSCleaned = pd.get_dummies(dfMSCleaned, columns=['New Plastic Type'], drop_first = False)\n",
    "dfMSCleaned = pd.get_dummies(dfMSCleaned, drop_first = True)\n",
    "dfMSCleaned = dfMSCleaned.drop(['New Plastic Type_0'], axis = 1)\n",
    "dfMSCleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPUTATION OF MISSING VALUES\n",
    "imputer = IterativeImputer (estimator = ExtraTreesRegressor(n_estimators=10, random_state=123), max_iter=50)\n",
    "n = imputer.fit_transform(dfMSCleaned)\n",
    "dfMSCleanedImputed = pd.DataFrame(n, columns = list(dfMSCleaned.columns))\n",
    "dfMSCleanedImputed.info()\n",
    "print ('There is '+str(sum(n < 0 for n in dfMSCleanedImputed.values.flatten()))+' negative values in the new Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSCleanedImputed['New Plastic Type_Nylon'] = dfMSCleanedImputed['New Plastic Type_Nylon'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PE'] = dfMSCleanedImputed['New Plastic Type_PE'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PET'] = dfMSCleanedImputed['New Plastic Type_PET'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PP'] = dfMSCleanedImputed['New Plastic Type_PP'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PU'] = dfMSCleanedImputed['New Plastic Type_PU'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PVC'] = dfMSCleanedImputed['New Plastic Type_PVC'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_Plastic Mix'] = dfMSCleanedImputed['New Plastic Type_Plastic Mix'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_e-waste'] = dfMSCleanedImputed['New Plastic Type_e-waste'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed = dfMSCleanedImputed.drop(['Plastic Addition by bitumen weight (%)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dfMSCleanedImputedScaled = pd.DataFrame(scaler.fit_transform(dfMSCleanedImputed), columns = list(dfMSCleanedImputed.columns))\n",
    "dfMSCleanedImputedScaled.to_clipboard()"
   ]
  },
  {
   "source": [
    "## 5. Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfMSCleanedImputedScaled.loc[:, dfMSCleanedImputedScaled.columns != 'MS of the sample (kN)']\n",
    "y = dfMSCleanedImputedScaled.loc[:,'MS of the sample (kN)']\n",
    "cv = RepeatedKFold(n_splits = 5, n_repeats = 10, random_state = 123)"
   ]
  },
  {
   "source": [
    "### 5.1 Model Evaluation\n",
    "#### Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'fit_intercept': [True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(LinearRegression(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit = 'neg_mean_squared_error', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Lasso Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.001,1, 10, 15, 30, 50, 100],\n",
    "            'fit_intercept':[True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(Lasso(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit = 'neg_mean_squared_error', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Ridge Linear regression model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0,5,15,100],\n",
    "'fit_intercept': [True, False],\n",
    "'solver': [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Linear Elastic net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.01,1,2,3,4],\n",
    "'fit_intercept': [True, False]}\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit = 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "'linearregression__fit_intercept': [True, False],\n",
    "'linearregression__positive':[True, False]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit = 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Lasso Polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Lasso(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "            'lasso__alpha': [1, 10, 15, 30, 50, 100],\n",
    "            'lasso__fit_intercept':[True, False],\n",
    "            'lasso__positive': [True, False],\n",
    "            'lasso__max_iter': [3000]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit = 'neg_mean_squared_error', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Ridge polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Ridge(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "'ridge__alpha':[10, 20,30,50],\n",
    "'ridge__fit_intercept': [True, False],\n",
    "'ridge__solver': [ 'lsqr', 'cholesky', 'sparse_cg', 'svd', 'sag']}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Support vector regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "param_grid = {\n",
    "    'kernel':['linear','rbf', 'sigmoid','poly'],\n",
    "    'degree':[2,3],\n",
    "    'C':[0.01,1,5,10],\n",
    "    'epsilon':[0.1,0.5, 1, 1.5]\n",
    "}\n",
    "grid = GridSearchCV(SVR(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#### Decision Tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth':[2,3,5,10],\n",
    "    'min_samples_split':[2,3,4],\n",
    "    'min_samples_leaf':[1,2]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(RandomForestRegressor(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit='neg_mean_squared_error', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### Extra tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(ExtraTreesRegressor(), param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit='neg_mean_squared_error', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "#### XG Boost Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoostModel = XGBRegressor()\n",
    "scores = cross_val_score(XGBoostModel, X, y , scoring = 'neg_mean_squared_error', cv = cv)\n",
    "scores = np.absolute(scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph employed for selecting important features during tunning\n",
    "XGBoostModel.fit(X,y)\n",
    "ax = plot_importance(XGBoostModel, height=0.8, importance_type='gain', show_values=False)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(10,10)"
   ]
  },
  {
   "source": [
    "## 6. Best Model Tunning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns = X.columns.astype(str)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "cv = RepeatedKFold(n_splits = 5, n_repeats = 15, random_state = 123)"
   ]
  },
  {
   "source": [
    "### 6.1. Feature selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method used in feature evaluation, it will return the R2 and MSE of the train and test set\n",
    "def evaluatefeatures (X = X, y = y):\n",
    "    cv_results = cross_validate(XGBRegressor(random_state = 1), X, y, cv = cv, scoring = ['r2', 'neg_mean_squared_error'], return_train_score = True)\n",
    "    print ('R2 in train set:' + str(np.average(cv_results['train_r2'])))\n",
    "    print ('MSE in train set:' + str(np.average(-cv_results['train_neg_mean_squared_error'])))\n",
    "    print ('R2 in test set:' + str(np.average(cv_results['test_r2'])))\n",
    "    print ('MSE in test set:' + str(np.average(-cv_results['test_neg_mean_squared_error'])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19']], y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "                        'Bitumen content in the sample']], y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "                        'Bitumen content in the sample',\n",
    "                        'Consolidated bitumen penetration grade_50/70',\n",
    "                        'Consolidated bitumen penetration grade_70/100',]], y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "                        'Bitumen content in the sample',\n",
    "                        'New Plastic Type_Nylon',\n",
    "                        'New Plastic Type_PE', \n",
    "                        'New Plastic Type_PET', \n",
    "                        'New Plastic Type_PP',\n",
    "                        'New Plastic Type_PU', \n",
    "                        'New Plastic Type_PVC',\n",
    "                        'New Plastic Type_Plastic Mix', \n",
    "                        'New Plastic Type_e-waste',\n",
    "                        ]], y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "                        'Bitumen content in the sample',\n",
    "                        'New Plastic Type_Nylon',\n",
    "                        'New Plastic Type_PE', \n",
    "                        'New Plastic Type_PET', \n",
    "                        'New Plastic Type_PP',\n",
    "                        'New Plastic Type_PU', \n",
    "                        'New Plastic Type_PVC',\n",
    "                        'New Plastic Type_Plastic Mix', \n",
    "                        'New Plastic Type_e-waste',\n",
    "                        'Aggregate absorption (%)'\n",
    "                        ]], y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "                        'Bitumen content in the sample',\n",
    "                        'New Plastic Type_Nylon',\n",
    "                        'New Plastic Type_PE', \n",
    "                        'New Plastic Type_PET', \n",
    "                        'New Plastic Type_PP',\n",
    "                        'New Plastic Type_PU', \n",
    "                        'New Plastic Type_PVC',\n",
    "                        'New Plastic Type_Plastic Mix', \n",
    "                        'New Plastic Type_e-waste',\n",
    "                        'Plastic particle size (mm)'\n",
    "                        ]], y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatefeatures( X = X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "                        'Bitumen content in the sample',\n",
    "                        'New Plastic Type_Nylon',\n",
    "                        'New Plastic Type_PE', \n",
    "                        'New Plastic Type_PET', \n",
    "                        'New Plastic Type_PP',\n",
    "                        'New Plastic Type_PU', \n",
    "                        'New Plastic Type_PVC',\n",
    "                        'New Plastic Type_Plastic Mix', \n",
    "                        'New Plastic Type_e-waste',\n",
    "                        'Mixing Process_Dry', \n",
    "                        'Mixing Process_Wet'\n",
    "                        ]], y = y_train)"
   ]
  },
  {
   "source": [
    "The features most approppiate for the model are aggregates gradation, bitumen content, plastic type, plastic addition.\n",
    "### 6.2 Model Tunning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_evaluation (parameters, X, y):\n",
    "    param_grid = parameters\n",
    "    grid = GridSearchCV(XGBRegressor(random_state = 1), param_grid, cv=cv, scoring=['neg_mean_squared_error', 'r2'], refit='neg_mean_squared_error')\n",
    "    grid.fit(X, y)\n",
    "    test_MSE = -grid.cv_results_['mean_test_neg_mean_squared_error'][grid.best_index_]\n",
    "    test_r2 = grid.cv_results_['mean_test_r2'][grid.best_index_]\n",
    "    best_param = grid.best_params_\n",
    "    print ('r2 test: ' + str(test_r2))\n",
    "    print ('MSE test: ' + str(test_MSE))\n",
    "    print ('Best Parameters ' + str(best_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  X_train[['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "        'Bitumen content in the sample',\n",
    "        'New Plastic Type_Nylon',\n",
    "        'New Plastic Type_PE', \n",
    "        'New Plastic Type_PET', \n",
    "        'New Plastic Type_PP',\n",
    "        'New Plastic Type_PU', \n",
    "        'New Plastic Type_PVC',\n",
    "        'New Plastic Type_Plastic Mix', \n",
    "        'New Plastic Type_e-waste',\n",
    "        ]]\n",
    "X_test = X_test [['0.075', '0.3', '0.6', '2.36', '4.75','9.5', '12.5', '19',\n",
    "        'Bitumen content in the sample',\n",
    "        'New Plastic Type_Nylon',\n",
    "        'New Plastic Type_PE', \n",
    "        'New Plastic Type_PET', \n",
    "        'New Plastic Type_PP',\n",
    "        'New Plastic Type_PU', \n",
    "        'New Plastic Type_PVC',\n",
    "        'New Plastic Type_Plastic Mix', \n",
    "        'New Plastic Type_e-waste',\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    }\n",
    "tuning_evaluation (param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':np.arange(3,11,1)\n",
    "    }\n",
    "tuning_evaluation (param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':np.arange(1,11,1)\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': np.arange(0,11,1)\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': [0],\n",
    "        'gamma' : [0, 0.001, 0.01, 0.1, 1, 10]\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': [0],\n",
    "        'gamma' : [0],\n",
    "        'subsample' : np.arange(0.5, 1.1, 0.1)\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': [0],\n",
    "        'gamma' : [0],\n",
    "        'subsample' : [1],\n",
    "        'colsample_bytree':[0, 0.5 ,1],\n",
    "        'colsample_bylevel':[0, 0.5 ,1],\n",
    "        'colsample_bynode':[0, 0.5 ,1]\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': [0],\n",
    "        'gamma' : [0],\n",
    "        'subsample' : [1],\n",
    "        'colsample_bytree':[1],\n",
    "        'colsample_bylevel':[1],\n",
    "        'colsample_bynode':[1],\n",
    "        'lambda' : np.arange(5,15,1)\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': [0],\n",
    "        'gamma' : [0],\n",
    "        'subsample' : [1],\n",
    "        'colsample_bytree':[1],\n",
    "        'colsample_bylevel':[1],\n",
    "        'colsample_bynode':[1],\n",
    "        'lambda' : [14],\n",
    "        'alpha' : np.arange(0,11,1)\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'eta':[0.4],\n",
    "        'max_depth':[6],\n",
    "        'min_child_weight':[7],\n",
    "        'max_delta_step': [0],\n",
    "        'gamma' : [0],\n",
    "        'subsample' : [1],\n",
    "        'colsample_bytree':[1],\n",
    "        'colsample_bylevel':[1],\n",
    "        'colsample_bynode':[1],\n",
    "        'lambda' : [14],\n",
    "        'alpha' : [0],\n",
    "        'tree_method' : ['auto']\n",
    "    }\n",
    "tuning_evaluation (param_grid, X = X_train, y = y_train)"
   ]
  },
  {
   "source": [
    "### 6.3 Final model evaluation on test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGModel = XGBRegressor(random_state = 123,\n",
    "                        eta = 0.4,\n",
    "                        max_depth = 6,\n",
    "                        min_child_weight =7,\n",
    "                        max_delta_step = 0,\n",
    "                        gamma = 0,\n",
    "                        subsample = 1,\n",
    "                        colsample_bytree = 1,\n",
    "                        colsample_bylevel = 1,\n",
    "                        colsample_bynode = 1, \n",
    "                        reg_lambda = 14,\n",
    "                        alpha = 0,\n",
    "                        tree_method = 'auto')\n",
    "XGModel.fit(X_train, y_train)\n",
    "predictions_test = XGModel.predict(X_test)\n",
    "r2_test = r2_score(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "print('The test r2 is: ' + str(r2_test))\n",
    "print('The test MSE is: ' + str(mse_test))\n",
    "predictions_train = XGModel.predict(X_train)\n",
    "r2_train = r2_score(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "print('The train r2 is: ' + str(r2_train))\n",
    "print('The train MSE is: ' + str(mse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(XGModel, height=0.8, importance_type = 'gain', show_values = False)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(6,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = pd.DataFrame(np.c_[np.array(X_train.columns), XGModel.feature_importances_], columns = ['feature', 'importance'])\n",
    "features_importance = features_importance.sort_values('importance', ascending = False, ignore_index = True)\n",
    "features_importance['sum'] = features_importance['importance'].cumsum()\n",
    "features_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_tree(XGModel)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}