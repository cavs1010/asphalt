{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('spyder': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "c84dbb84446594c7c33060e8d4d00ab472a22f3e722bb969e9e6b8b01c6ee377"
   }
  },
  "interpreter": {
   "hash": "fc22dc884bcedff190aa0e4687df8895271c55103986e14a4799605b50a1eed1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Asphalt mix prediction after plastic addition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% IMPORTS\n",
    "#BASICS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import absolute\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import display, Markdown, Latex\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#STATISTICS\n",
    "from scipy.stats import normaltest\n",
    "from scipy import stats\n",
    "\n",
    "#ML TRAINING AND DATA PREPROCESSING\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#ML MODELS\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "#MODEL EVALUATION\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#METRICS\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "source": [
    "## Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate Outliers based on the interquantile\n",
    "#datFrame: Data frame where the outliers will be eliminated.\n",
    "#columnName: the name of the column where the outliers will be identified.\n",
    "def eliminateOutliers (dataFrame, columnName):\n",
    "    Q1 = dataFrame[columnName].quantile(0.25)\n",
    "    Q3 = dataFrame[columnName].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    print('Initial dataframe size: '+str(dataFrame.shape))\n",
    "    dataFrame = dataFrame[(dataFrame[columnName] < (Q3 + 1.5 * IQR)) & (dataFrame[columnName] > (Q1 - 1.5 * IQR))]\n",
    "    print('Final dataframe size: '+str(dataFrame.shape))\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot graphs for the categorical variables\n",
    "# dataFrame: Data frame associated to the property of interest (dfAirVoids, dfMS, dfMF, dfITS, dfTSR)\n",
    "# propertyOfInterest: the name of the column where the property of interest is located.\n",
    "# columnName1...4: The categorical columns to evaluate.\n",
    "def displayBoxPlotGraphs (dataFrame, propertyOfInterest, columnName1, columnName2, columnName3, columnName4):\n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15,10))\n",
    "    sns.boxplot(y = propertyOfInterest, x = columnName1, data=dataFrame,  orient='v' , ax=ax1)\n",
    "    sns.boxplot(y = propertyOfInterest, x = columnName2, data=dataFrame,  orient='v' , ax=ax2)\n",
    "    sns.boxplot(y = propertyOfInterest, x= columnName3, data=dataFrame,  orient='v' , ax=ax3)\n",
    "    sns.boxplot(y= propertyOfInterest, x= columnName4, data=dataFrame,  orient='v' , ax=ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that print the best parameters, R2 and MSE based on a grid search.\n",
    "def printBestModel (grid):\n",
    "    mse = -grid.best_score_\n",
    "    print('Best Parameters:' , grid.best_params_)\n",
    "    print('Best MSE:' + str(mse))"
   ]
  },
  {
   "source": [
    "##  Data Import "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%DATA READING AND INITIAL PREPROCESSING\n",
    "numericColumns = ['Aggregate absorption (%)',\n",
    "                  'Apparent specific gravity',\n",
    "                    0.075,\n",
    "                    0.3,\n",
    "                    0.6,\n",
    "                    2.36,\n",
    "                    4.75,\n",
    "                    9.5,\n",
    "                    12.5,\n",
    "                    19,\n",
    "                    'Plastic particle size (mm)',\n",
    "                    'Mixing speed (RPM)',\n",
    "                    'Mixing Temperature',\n",
    "                    'Mixing Time (hours)',\n",
    "                    'Plastic Addition by bitumen weight (%)',\n",
    "                    'Bitumen content in the sample'\n",
    "                    ]\n",
    "categoricalColumns = ['Modified asphalt Mix?',\n",
    "                      'Agreggate Type',\n",
    "                    'Aggregate absorption [%]',\n",
    "                    'Filler used',\n",
    "                    'Consolidated bitumen penetration grade',\n",
    "                    'New Plastic Type',\n",
    "                    'Plastic pretreatment',\n",
    "                    'Plastic shape',\n",
    "                    'Plastic Size',\n",
    "                    'Mixing Process',\n",
    "                    'Aggregates replacement ?',\n",
    "                    'Bitumen replacement?',\n",
    "                    'Filler replacement',\n",
    "                    'Property',\n",
    "                    'Units']\n",
    "#It returns the dataframe of interes based on the property - 'AirVoids', 'MS', 'MF', 'ITS', 'TSR'\n",
    "def returnDf (propertyOfInterest):\n",
    "    df = pd.read_excel('fileML.xlsx', sheet_name = propertyOfInterest, engine='openpyxl')\n",
    "    df = df.set_index(propertyOfInterest + ' ID')\n",
    "    df.loc[:,:'Units'] = df.loc[:,:'Units'].applymap(str)\n",
    "    df.loc[:,:'Units'] = df.loc[:,:'Units'] .applymap(str.strip)\n",
    "    df.replace('NS', np.nan, inplace = True)\n",
    "    df[numericColumns] = df[numericColumns].replace('N/a', 0).astype(float)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoids = returnDf('AirVoids')\n",
    "dfMS = returnDf('MS')\n",
    "dfMF = returnDf('MF')\n",
    "dfITS = returnDf('ITS')\n",
    "dfTSR = returnDf('TSR')"
   ]
  },
  {
   "source": [
    "-------------------------------\n",
    "#  1. Air Voids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial dataframe size: (305, 34)\nFinal dataframe size: (288, 34)\n"
     ]
    }
   ],
   "source": [
    "dfAirVoids = eliminateOutliers(dfAirVoids, 'Air voids of the sample (%)')"
   ]
  },
  {
   "source": [
    "## 1.1 Data Exploration\n",
    "###  1.1.1 Total Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInteres = numericColumns[0:2]+numericColumns[10:]+['Air voids of the sample (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoids.describe(include = 'all')"
   ]
  },
  {
   "source": [
    "In total, there is $\\color{red}{\\text{288}}$ observations. Let's see if there is any correlation with the continual and categorical variables. The mean of $\\color{red}{\\text{Air voids}}$, which is the common value for the optimum asphalt mix design."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfAirVoids[['Aggregate absorption [%]', 'Apparent specific gravity', 'Bitumen content in the sample', 'Air voids of the sample (%)']], figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "It is possible to see a negative correlation between $\\color{red}{\\text{Bitumen Content}}$  and $\\color{red}{\\text{Air Voids}}$. This makes sense because higher bitumen quantities will fill more voids in the mixture.\n",
    "\n",
    "Among the categorical variables, we will initially focus on the general ones that might have an effect on air voids: $\\color{red}{\\text{Aggregate type}}$, $\\color{red}{\\text{Filler used}}$ and $\\color{red}{\\text{Bitumen Grade}}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfAirVoids, propertyOfInterest = \"Air voids of the sample (%)\", columnName1 = \"Agreggate Type\", columnName2 = \"Filler used\", columnName3 = \"Consolidated bitumen penetration grade\", columnName4 = \"Modified asphalt Mix?\")"
   ]
  },
  {
   "source": [
    "Within the  $\\color{red}{\\text{Aggregate type}}$, although it exists differences -especially with the Ophitic- this data migh not be robust enough due to the sample size of this type of aggregate. It happens the same with the  $\\color{red}{\\text{Bitumen type}}$, where only  $\\color{purple}{\\text{5}}$ observations used the 40/50 bitumen. The  $\\color{red}{\\text{addition of plastic}}$ seems to have not strong influence in the air voids."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION:\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfAirVoids.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap Air Voids', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "source": [
    "###  1.1.2 Modified mixtures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoidsModvsUnmod = dfAirVoids [['Modified asphalt Mix?', 'Air voids of the sample (%)']]\n",
    "dfAirVoidsModvsUnmod.groupby(['Modified asphalt Mix?'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoidsModified = dfAirVoids[dfAirVoids['Modified asphalt Mix?']=='Yes']\n",
    "dfAirVoidsModified.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfAirVoidsModified[columnsOfInteres], figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION:\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfAirVoidsModified.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfAirVoidsModified, propertyOfInterest = \"Air voids of the sample (%)\", columnName1 = \"Agreggate Type\", columnName2 = \"Plastic shape\", columnName3 = \"New Plastic Type\", columnName4 = \"Mixing Process\")"
   ]
  },
  {
   "source": [
    "Not much difference among dry and wet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###  1.1.3 Wet vs. Dry Mixing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoidsWetvsDry = dfAirVoidsModified [['Mixing Process', 'Air voids of the sample (%)']]\n",
    "dfAirVoidsWetvsDry.groupby(['Mixing Process'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfAirVoidsModified[columnsOfInteres+['Mixing Process']], hue=\"Mixing Process\", height=2.5)"
   ]
  },
  {
   "source": [
    "##  **Air Voids summary:**\n",
    "\n",
    "*   There are missing values mainly in $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{Aggregate type}}$ and $\\color{red}{\\text{filler used}}$.\n",
    "*   Total sample: # observations = 288, Mean = 4, Standard deviation = 0.98 \n",
    "*   Sample with asphalt modified: # observations = 228, Mean = 3.94, Standard deviation = 1.017\n",
    "*   It seems that there will be a strong correlation with $\\color{red}{\\text{Aggregates gradation}}$.\n",
    "*   Dry vs. Wet:\n",
    "    *   Not strong difference between dry and wet.\n",
    "    *   Mean in wet = 3.64, Mean in dry = 4.16.\n",
    "\n",
    "\n",
    "## 1.2 Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 288 entries, 1 to 305\nData columns (total 34 columns):\n #   Column                                  Non-Null Count  Dtype  \n---  ------                                  --------------  -----  \n 0   Article ID                              288 non-null    object \n 1   Global ID                               288 non-null    object \n 2   Modified asphalt Mix?                   288 non-null    object \n 3   Agreggate Type                          197 non-null    object \n 4   Aggregate absorption (%)                231 non-null    float64\n 5   Apparent specific gravity               77 non-null     float64\n 6   0.075                                   254 non-null    float64\n 7   0.3                                     254 non-null    float64\n 8   0.6                                     251 non-null    float64\n 9   2.36                                    258 non-null    float64\n 10  4.75                                    258 non-null    float64\n 11  9.5                                     251 non-null    float64\n 12  12.5                                    243 non-null    float64\n 13  19                                      258 non-null    float64\n 14  Filler used                             122 non-null    object \n 15  Bitumen Type Penetration Grade          288 non-null    object \n 16  Consolidated bitumen penetration grade  288 non-null    object \n 17  New Plastic Type                        264 non-null    object \n 18  Plastic pretreatment                    288 non-null    object \n 19  Plastic shape                           288 non-null    object \n 20  Plastic Size                            248 non-null    object \n 21  Plastic particle size (mm)              238 non-null    float64\n 22  Mixing Process                          288 non-null    object \n 23  Mixing speed (RPM)                      277 non-null    float64\n 24  Mixing Temperature                      285 non-null    float64\n 25  Mixing Time (hours)                     277 non-null    float64\n 26  Aggregates replacement ?                288 non-null    object \n 27  Bitumen replacement?                    288 non-null    object \n 28  Filler replacement                      288 non-null    object \n 29  Plastic Addition by bitumen weight (%)  288 non-null    float64\n 30  Property                                288 non-null    object \n 31  Units                                   288 non-null    object \n 32  Bitumen content in the sample           288 non-null    float64\n 33  Air voids of the sample (%)             288 non-null    float64\ndtypes: float64(17), object(17)\nmemory usage: 78.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dfAirVoids.info()"
   ]
  },
  {
   "source": [
    "*   Process for dealing with missing categorical values:\n",
    "\n",
    "    1.  I will eliminate the rows with Nan values in the column $\\color{red}{\\text{New Plastic Type}}$. In total, there is 24 missing values, which represent 8.33% of the sample\n",
    "    2.  Replace the 'N/a' value when there is no modifiation of the asphalt mix.\n",
    "\n",
    "*   Process for dealing with missing numeric values:\n",
    "\n",
    "    *   The imputer used was the multivariable imputer, and the estimator was the ExtraTreeRegressor ([link][1]).\n",
    "    By using this estimator, I did not get any negative values among the missing features, and it presents a low MSE value.\n",
    "    \n",
    "[1]:https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Variables\n",
    "dfAirVoidsCleaned = dfAirVoids.drop(['Article ID', \n",
    "                                    'Global ID',\n",
    "                                    'Modified asphalt Mix?',\n",
    "                                    'Agreggate Type', \n",
    "                                    'Apparent specific gravity', \n",
    "                                    'Filler used', \n",
    "                                    'Bitumen Type Penetration Grade', \n",
    "                                    'Property', \n",
    "                                    'Units', \n",
    "                                    'Plastic Size' ], axis = 1)\n",
    "dfAirVoidsCleaned = dfAirVoidsCleaned.replace('N/a', 0)\n",
    "dfAirVoidsCleaned = dfAirVoidsCleaned.dropna(subset=['New Plastic Type'])\n",
    "dfAirVoidsCleaned = pd.get_dummies(dfAirVoidsCleaned, columns=['New Plastic Type'], drop_first=False)\n",
    "dfAirVoidsCleaned = pd.get_dummies(dfAirVoidsCleaned, drop_first=True)\n",
    "dfAirVoidsCleaned = dfAirVoidsCleaned.drop(['New Plastic Type_0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There is 0 negative values in the new Dataframe\n"
     ]
    }
   ],
   "source": [
    "#IMPUTATION OF MISSING VALUES\n",
    "imputer = IterativeImputer (estimator = ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=50)\n",
    "n = imputer.fit_transform(dfAirVoidsCleaned)\n",
    "dfAirVoidsCleanedImputed = pd.DataFrame(n, columns = list(dfAirVoidsCleaned.columns))\n",
    "print ('There is '+str(sum(n < 0 for n in dfAirVoidsCleanedImputed.values.flatten()))+' negative values in the new Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAirVoidsCleanedImputed['New Plastic Type_Nylon'] = dfAirVoidsCleanedImputed['New Plastic Type_Nylon'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed['New Plastic Type_PE'] = dfAirVoidsCleanedImputed['New Plastic Type_PE'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed['New Plastic Type_PET'] = dfAirVoidsCleanedImputed['New Plastic Type_PET'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed['New Plastic Type_PP'] = dfAirVoidsCleanedImputed['New Plastic Type_PP'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed['New Plastic Type_PS'] = dfAirVoidsCleanedImputed['New Plastic Type_PS'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed['New Plastic Type_PU'] = dfAirVoidsCleanedImputed['New Plastic Type_PU'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed['New Plastic Type_Plastic Mix'] = dfAirVoidsCleanedImputed['New Plastic Type_Plastic Mix'] * dfAirVoidsCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfAirVoidsCleanedImputed = dfAirVoidsCleanedImputed.drop(['Plastic Addition by bitumen weight (%)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Feature Scaling\n",
    " scaler = MinMaxScaler()\n",
    " dfAirVoidsCleanedImputedScaled = pd.DataFrame(scaler.fit_transform(dfAirVoidsCleanedImputed), columns = list(dfAirVoidsCleanedImputed.columns))\n",
    " dfAirVoidsCleanedImputedScaled.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 264 entries, 0 to 263\nData columns (total 33 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption (%)                       264 non-null    float64\n 1   0.075                                          264 non-null    float64\n 2   0.3                                            264 non-null    float64\n 3   0.6                                            264 non-null    float64\n 4   2.36                                           264 non-null    float64\n 5   4.75                                           264 non-null    float64\n 6   9.5                                            264 non-null    float64\n 7   12.5                                           264 non-null    float64\n 8   19                                             264 non-null    float64\n 9   Plastic particle size (mm)                     264 non-null    float64\n 10  Mixing speed (RPM)                             264 non-null    float64\n 11  Mixing Temperature                             264 non-null    float64\n 12  Mixing Time (hours)                            264 non-null    float64\n 13  Bitumen content in the sample                  264 non-null    float64\n 14  Air voids of the sample (%)                    264 non-null    float64\n 15  New Plastic Type_Nylon                         264 non-null    float64\n 16  New Plastic Type_PE                            264 non-null    float64\n 17  New Plastic Type_PET                           264 non-null    float64\n 18  New Plastic Type_PP                            264 non-null    float64\n 19  New Plastic Type_PS                            264 non-null    float64\n 20  New Plastic Type_PU                            264 non-null    float64\n 21  New Plastic Type_Plastic Mix                   264 non-null    float64\n 22  Consolidated bitumen penetration grade_50/70   264 non-null    float64\n 23  Consolidated bitumen penetration grade_70/100  264 non-null    float64\n 24  Plastic pretreatment_Physical                  264 non-null    float64\n 25  Plastic pretreatment_Plastic Melted            264 non-null    float64\n 26  Plastic shape_Fibers                           264 non-null    float64\n 27  Plastic shape_Pellets                          264 non-null    float64\n 28  Plastic shape_Shredded                         264 non-null    float64\n 29  Mixing Process_Dry                             264 non-null    float64\n 30  Mixing Process_Wet                             264 non-null    float64\n 31  Aggregates replacement ?_Yes                   264 non-null    float64\n 32  Bitumen replacement?_Yes                       264 non-null    float64\ndtypes: float64(33)\nmemory usage: 68.2 KB\n"
     ]
    }
   ],
   "source": [
    "dfAirVoidsCleanedImputed.info()"
   ]
  },
  {
   "source": [
    "## 1.3 Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfAirVoidsCleanedImputedScaled.loc[:, dfAirVoidsCleanedImputedScaled.columns != 'Air voids of the sample (%)']\n",
    "y = dfAirVoidsCleanedImputedScaled.loc[:,'Air voids of the sample (%)']\n",
    "cv = RepeatedKFold(n_splits = 5, n_repeats = 10, random_state = 123)"
   ]
  },
  {
   "source": [
    "### Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'fit_intercept': False, 'positive': True}\nBest MSE:0.02127826485158443\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'fit_intercept': [True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(LinearRegression(), param_grid, cv = cv, scoring=['neg_mean_squared_error'], refit = 'neg_mean_squared_error', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Lasso Linear Model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\cavs1\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008001737089143202, tolerance: 0.005551244305576722\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "Best Parameters: {'alpha': 0.001, 'fit_intercept': True, 'positive': False}\n",
      "Best MSE:0.016905414665606633\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [0.001,1, 10, 15, 30, 50, 100],\n",
    "            'fit_intercept':[True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(Lasso(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit = 'neg_mean_squared_error', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Ridge linear regression model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'alpha': 7, 'fit_intercept': True, 'solver': 'lsqr'}\nBest MSE:0.019962138197387954\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [7, 8, 10,100],\n",
    "'fit_intercept': [True, False],\n",
    "'solver': [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "grid = GridSearchCV(Ridge(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit = 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Linear Elastic Net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'alpha': 0.01, 'fit_intercept': False}\nBest MSE:0.022661376417920164\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [0.01,1,2,3,4],\n",
    "'fit_intercept': [True, False]}\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit = 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Polynomial Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'linearregression__fit_intercept': False, 'linearregression__positive': True, 'polynomialfeatures__degree': 3}\nBest MSE:0.019340664389902496\n"
     ]
    }
   ],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "'linearregression__fit_intercept': [True, False],\n",
    "'linearregression__positive':[True, False]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit = 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Lasso Polynomial Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'lasso__alpha': 1, 'lasso__fit_intercept': True, 'lasso__max_iter': 2000, 'lasso__positive': True, 'polynomialfeatures__degree': 2}\nBest MSE:0.03716195618960517\n"
     ]
    }
   ],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Lasso(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "            'lasso__alpha': [1,2, 3, 10, 15, 30],\n",
    "            'lasso__fit_intercept':[True, False],\n",
    "            'lasso__positive': [True, False],\n",
    "            'lasso__max_iter': [2000,3000, 3500]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit = 'neg_mean_squared_error', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Ridge polynomial regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'polynomialfeatures__degree': 3, 'ridge__alpha': 20, 'ridge__fit_intercept': True, 'ridge__solver': 'cholesky'}\nBest MSE:0.015158402406889983\n"
     ]
    }
   ],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Ridge(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "'ridge__alpha':[20,30,50, 60],\n",
    "'ridge__fit_intercept': [True, False],\n",
    "'ridge__solver': [ 'lsqr', 'cholesky', 'sparse_cg', 'auto']}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Support vector regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'C': 5, 'degree': 2, 'epsilon': 0.1, 'kernel': 'rbf'}\nBest MSE:0.01360289289494892\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'kernel':['linear','rbf', 'sigmoid', 'poly'],\n",
    "    'degree':[2,3,4],\n",
    "    'C':[0.01,1,5,10],\n",
    "    'epsilon':[0.1,0.2, 1, 1.5]\n",
    "}\n",
    "grid = GridSearchCV(SVR(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Decision tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2}\nBest MSE:0.014393275608157063\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth':[1,2,3,5,10,30],\n",
    "    'min_samples_split':[2,3,4],\n",
    "    'min_samples_leaf':[0.4,1,2]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=cv, scoring=['neg_mean_squared_error'], refit='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Random Forest\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-9ece05919d38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                         \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                         )\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprintBestModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             random_state=self.random_state))\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \"\"\"\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1253\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "grid = RandomizedSearchCV(RandomForestRegressor(), \n",
    "                        random_grid, \n",
    "                        cv=cv, \n",
    "                        scoring=['neg_mean_squared_error'], \n",
    "                        refit='neg_mean_squared_error', \n",
    "                        n_iter=100,\n",
    "                        )\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Extra tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameters: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 10, 'bootstrap': False}\nBest MSE:0.031187924086990264\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(ExtraTreesRegressor(), param_grid, cv=10, scoring=['neg_mean_squared_error'], refit='neg_mean_squared_error', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### XG Boost Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.008176585080326452\n"
     ]
    }
   ],
   "source": [
    "XGBoostModel = XGBRegressor()\n",
    "\n",
    "scores = cross_val_score(XGBoostModel, X, y , scoring = 'neg_mean_squared_error', cv = cv)\n",
    "scores = np.absolute(scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "source": [
    "## Best Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 of train set is 0.9259274898735125\nMSE of the train set is 0.002751729378795027\nR2 of the test set is 0.7023939120970419\nMSE of the test set is 0.010344688105564746\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE REGRESSOR\n",
    "dtRegressor = DecisionTreeRegressor(max_depth=30, min_samples_leaf=2, min_samples_split=3)\n",
    "dtRegressor.fit(X_train, y_train)\n",
    "y_predict = dtRegressor.predict(X_test)\n",
    "y_predict_train = dtRegressor.predict(X_train)\n",
    "print('R2 of train set is ' + str(r2_score(y_train, y_predict_train)))\n",
    "print('MSE of the train set is ' + str (mean_squared_error(y_train, y_predict_train)))\n",
    "print ('R2 of the test set is ' + str(r2_score(y_test, y_predict)))\n",
    "print ('MSE of the test set is ' + str(mean_squared_error(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 of train set is 0.9966329237013066\nMSE of the train set is 0.00012508395835293665\nR2 of test set is 0.8285692544340779\nMSE of the test set is 0.005958875394921866\n"
     ]
    }
   ],
   "source": [
    "#XGB REGRESSOR\n",
    "XGRegressor = XGBRegressor()\n",
    "XGRegressor.fit(X_train, y_train)\n",
    "y_predict = XGRegressor.predict(X_test)\n",
    "y_predict_train = XGRegressor.predict(X_train)\n",
    "print('R2 of train set is ' + str(r2_score(y_train, y_predict_train)))\n",
    "print('MSE of the train set is ' + str (mean_squared_error(y_train, y_predict_train)))\n",
    "print('R2 of test set is ' + str(r2_score(y_test, y_predict)))\n",
    "print ('MSE of the test set is ' + str(mean_squared_error(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Aggregate absorption (%)', '0.075', '0.3', '0.6', '2.36', '4.75',\n",
       "       '9.5', '12.5', '19', 'Plastic particle size (mm)', 'Mixing speed (RPM)',\n",
       "       'Mixing Temperature', 'Mixing Time (hours)',\n",
       "       'Bitumen content in the sample', 'New Plastic Type_Nylon',\n",
       "       'New Plastic Type_PE', 'New Plastic Type_PET', 'New Plastic Type_PP',\n",
       "       'New Plastic Type_PS', 'New Plastic Type_PU',\n",
       "       'New Plastic Type_Plastic Mix',\n",
       "       'Consolidated bitumen penetration grade_50/70',\n",
       "       'Consolidated bitumen penetration grade_70/100',\n",
       "       'Plastic pretreatment_Physical', 'Plastic pretreatment_Plastic Melted',\n",
       "       'Plastic shape_Fibers', 'Plastic shape_Pellets',\n",
       "       'Plastic shape_Shredded', 'Mixing Process_Dry', 'Mixing Process_Wet',\n",
       "       'Aggregates replacement ?_Yes', 'Bitumen replacement?_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4649855982496962\n"
     ]
    }
   ],
   "source": [
    "XGBoostModel = XGBRegressor(seed = 123)\n",
    "cv = RepeatedKFold(n_splits = 10, n_repeats = 10)\n",
    "scores = cross_val_score(XGBoostModel, X[['0.075', \n",
    "                                            '0.3', \n",
    "                                            '0.6', \n",
    "                                            '2.36', \n",
    "                                            '4.75',\n",
    "                                            '9.5', \n",
    "                                            '12.5', \n",
    "                                            '19']], y , scoring = 'r2', cv = cv)\n",
    "scores = np.absolute(scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "source": [
    "-------------------------------\n",
    "#  2. Marshall Stability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial dataframe size: (406, 34)\nFinal dataframe size: (402, 34)\n"
     ]
    }
   ],
   "source": [
    "dfMS = eliminateOutliers(dfMS, 'MS of the sample (kN)')"
   ]
  },
  {
   "source": [
    "## 2.1 Data Exploration\n",
    "###  2.1.1 Total Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS.describe(include = \"all\")\n",
    "dfMS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS.iloc[:,2:].describe(include = 'all')"
   ]
  },
  {
   "source": [
    "I might have a problem with the $\\color{red}{\\text{Aggregate absorption}}$ because more than 20% of the data is missing. Regarding the $\\color{red}{\\text{MS}}$, there is a high dispersion ($\\sigma$ = 4.56), and the Mean seems normal. According to the Australian standards, the minimum value of the Marshall stability is between two and eigth."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfMS[['Aggregate absorption [%]', 'Apparent specific gravity', 'Bitumen content in the sample', 'MS of the sample (kN)']], figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfMS.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap MS', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "source": [
    "Interestingly, there is positive correlation in $\\color{red}{\\text{MS-Apparent specific gravity}}$ and $\\color{red}{\\text{MS-plastic addition by bitumen content}}$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfMS, propertyOfInterest = \"MS of the sample (kN)\", columnName1 = \"Agreggate Type\", columnName2 = \"Filler used\", columnName3 = \"Consolidated bitumen penetration grade\", columnName4 = \"Modified asphalt Mix?\")"
   ]
  },
  {
   "source": [
    "*   As it happened with the Air Voids, it exists a MS difference among the samples that employed the bitumen 40/50; however, it is important to note that the sample size for this group was not representative enough.\n",
    "\n",
    "*   Samples with plastic modification tend to have higher MS. The glue effect of the plastic and the stiffness increase of the bitumen might serve as valid explanations.\n",
    "\n",
    "*   No signigicant difference among the aggregate types and fillers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###  2.1.2 Modified mixtures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSModvsUnmod = dfMS [['Modified asphalt Mix?', 'MS of the sample (kN)']]\n",
    "dfMSModvsUnmod.groupby(['Modified asphalt Mix?'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSModified = dfMS[dfMS['Modified asphalt Mix?'] == 'Yes']\n",
    "dfMSModified.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInteres = numericColumns[0:2]+numericColumns[10:]+['MS of the sample (kN)']\n",
    "scatter_matrix(dfMSModified[columnsOfInteres], figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfMSModified.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap MS', fontdict={'fontsize':12}, pad=12)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    " $\\color{red}{\\text{MS-Apparent specific gravity}}$ presents the highest correlation with  $\\color{red}{\\text{MS}}$; however, it only has 66 observations, so it is not a convincing result. Other parameters such as  $\\color{red}{\\text{Plastic content}}$, and  $\\color{red}{\\text{gradation}}$ present an slight effect on the MS."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfMSModified, propertyOfInterest = \"MS of the sample (kN)\", columnName1 = \"Agreggate Type\", columnName2 = \"Plastic shape\", columnName3 = \"New Plastic Type\", columnName4 = \"Mixing Process\")"
   ]
  },
  {
   "source": [
    "The mean of the **dry** and **wet** process are not significantly different."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###  2.1.3 Wet vs. Dry Mixing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSWetvsDry = dfMSModified [['Mixing Process', 'MS of the sample (kN)']]\n",
    "dfMSWetvsDry.groupby(['Mixing Process'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfMSModified[columnsOfInteres+['Mixing Process']], hue=\"Mixing Process\", height=2.5)"
   ]
  },
  {
   "source": [
    "##  **MS summary:**\n",
    "\n",
    "*   There are missing values mainly in $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{Aggregate type}}$ and $\\color{red}{\\text{filler used}}$.\n",
    "*   Four outliers were eliminated. The final total sample included 402 data points ($\\mu$ = 14.47, $\\sigma$ = 4.6). \n",
    "*   $\\color{red}{\\text{Aggregate absorption}}$ seems to be a critical variable to include, but the percentage of missing values is more than 20%.\n",
    "*   $\\color{red}{\\text{Apparent specific gravity}}$ presents the strongest positive correlation with the Marshall stability, but it is not a reliable inference becasue it presents many missing points (318 missing points).\n",
    "* Although Marshall stability of modified asphalts is relatively higher than not modified, this is not certain because the high variances of both sample groups. $\\mu_{modified}$ = 15.12 vs. $\\mu_{unmodified}$ = 11.97\n",
    "*   $\\color{red}{\\text{Percentage of plastic addition}}$ has a noticeable possitive correlation with MS. (r = 0.39) \n",
    "*   MS of dry and wet are really similar -> $\\mu_{Dry}$ = 15.05 (200 observations) vs $\\mu_{Wet}$ = 15.2 (119 observations)\n",
    "\n",
    "## 2.2 Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMS.info()"
   ]
  },
  {
   "source": [
    "###  Pre-processing:\n",
    "1.  Eliminate the columns $\\color{red}{\\text{Article ID}}$, $\\color{red}{\\text{Global ID}}$, $\\color{red}{\\text{Aggregate type}}$, $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{filler used}}$, $\\color{red}{\\text{Bitumen type penetration}}$, $\\color{red}{\\text{Property}}$, $\\color{red}{\\text{plastic size}}$ and $\\color{red}{\\text{Units}}$.\n",
    "2.  Change the N/a to zero. This is for the unmodified mixtures.\n",
    "3.  Eliminate rows with missing values in $\\color{red}{\\text{New Plastic Type}}$, $\\color{red}{\\text{Plastic addition by bitumen weight}}$ and $\\color{red}{\\text{bitumen}}$ content in sample\n",
    "4.  Change categorical columns to numeric.\n",
    "5.  Imputer to $\\color{red}{\\text{Aggregate absorption}}$, $\\color{red}{\\text{gradation}}$, $\\color{red}{\\text{plastic size(mm)}}$, and $\\color{red}{\\text{mixing parameters}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 374 entries, 1 to 406\nData columns (total 35 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption [%]                       217 non-null    float64\n 1   0.075                                          322 non-null    float64\n 2   0.3                                            369 non-null    float64\n 3   0.6                                            341 non-null    float64\n 4   2.36                                           352 non-null    float64\n 5   4.75                                           369 non-null    float64\n 6   9.5                                            341 non-null    float64\n 7   12.5                                           354 non-null    float64\n 8   19                                             369 non-null    float64\n 9   Plastic particle size (mm)                     304 non-null    float64\n 10  Mixing speed (RPM)                             343 non-null    float64\n 11  Mixing Temperature                             357 non-null    float64\n 12  Mixing Time (hours)                            344 non-null    float64\n 13  Plastic Addition by bitumen weight (%)         374 non-null    float64\n 14  Bitumen content in the sample                  374 non-null    float64\n 15  MS of the sample (kN)                          374 non-null    float64\n 16  New Plastic Type_Nylon                         374 non-null    uint8  \n 17  New Plastic Type_PE                            374 non-null    uint8  \n 18  New Plastic Type_PET                           374 non-null    uint8  \n 19  New Plastic Type_PP                            374 non-null    uint8  \n 20  New Plastic Type_PU                            374 non-null    uint8  \n 21  New Plastic Type_PVC                           374 non-null    uint8  \n 22  New Plastic Type_Plastic Mix                   374 non-null    uint8  \n 23  New Plastic Type_e-waste                       374 non-null    uint8  \n 24  Consolidated bitumen penetration grade_50/70   374 non-null    uint8  \n 25  Consolidated bitumen penetration grade_70/100  374 non-null    uint8  \n 26  Plastic pretreatment_Physical                  374 non-null    uint8  \n 27  Plastic pretreatment_Plastic Melted            374 non-null    uint8  \n 28  Plastic shape_Fibers                           374 non-null    uint8  \n 29  Plastic shape_Pellets                          374 non-null    uint8  \n 30  Plastic shape_Shredded                         374 non-null    uint8  \n 31  Mixing Process_Dry                             374 non-null    uint8  \n 32  Mixing Process_Wet                             374 non-null    uint8  \n 33  Aggregates replacement ?_Yes                   374 non-null    uint8  \n 34  Bitumen replacement?_Yes                       374 non-null    uint8  \ndtypes: float64(16), uint8(19)\nmemory usage: 56.6 KB\n"
     ]
    }
   ],
   "source": [
    "#Categorical Variables\n",
    "dfMSCleaned = dfMS.drop(['Article ID', \n",
    "                        'Global ID',\n",
    "                        'Modified asphalt Mix?',\n",
    "                        'Agreggate Type', \n",
    "                        'Apparent specific gravity', \n",
    "                        'Filler used', \n",
    "                        'Bitumen Type Penetration Grade', \n",
    "                        'Property', \n",
    "                        'Units', \n",
    "                        'Plastic Size' ], axis = 1)\n",
    "dfMSCleaned = dfMSCleaned.replace('N/a', 0)\n",
    "dfMSCleaned = dfMSCleaned.dropna(subset=['New Plastic Type', \n",
    "                                        'Plastic Addition by bitumen weight (%)', \n",
    "                                        'Bitumen content in the sample'])\n",
    "dfMSCleaned = pd.get_dummies(dfMSCleaned, columns=['New Plastic Type'], drop_first = False)\n",
    "dfMSCleaned = pd.get_dummies(dfMSCleaned, drop_first = True)\n",
    "dfMSCleaned = dfMSCleaned.drop(['New Plastic Type_0'], axis = 1)\n",
    "dfMSCleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 374 entries, 0 to 373\n",
      "Data columns (total 35 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   Aggregate absorption [%]                       374 non-null    float64\n",
      " 1   0.075                                          374 non-null    float64\n",
      " 2   0.3                                            374 non-null    float64\n",
      " 3   0.6                                            374 non-null    float64\n",
      " 4   2.36                                           374 non-null    float64\n",
      " 5   4.75                                           374 non-null    float64\n",
      " 6   9.5                                            374 non-null    float64\n",
      " 7   12.5                                           374 non-null    float64\n",
      " 8   19                                             374 non-null    float64\n",
      " 9   Plastic particle size (mm)                     374 non-null    float64\n",
      " 10  Mixing speed (RPM)                             374 non-null    float64\n",
      " 11  Mixing Temperature                             374 non-null    float64\n",
      " 12  Mixing Time (hours)                            374 non-null    float64\n",
      " 13  Plastic Addition by bitumen weight (%)         374 non-null    float64\n",
      " 14  Bitumen content in the sample                  374 non-null    float64\n",
      " 15  MS of the sample (kN)                          374 non-null    float64\n",
      " 16  New Plastic Type_Nylon                         374 non-null    float64\n",
      " 17  New Plastic Type_PE                            374 non-null    float64\n",
      " 18  New Plastic Type_PET                           374 non-null    float64\n",
      " 19  New Plastic Type_PP                            374 non-null    float64\n",
      " 20  New Plastic Type_PU                            374 non-null    float64\n",
      " 21  New Plastic Type_PVC                           374 non-null    float64\n",
      " 22  New Plastic Type_Plastic Mix                   374 non-null    float64\n",
      " 23  New Plastic Type_e-waste                       374 non-null    float64\n",
      " 24  Consolidated bitumen penetration grade_50/70   374 non-null    float64\n",
      " 25  Consolidated bitumen penetration grade_70/100  374 non-null    float64\n",
      " 26  Plastic pretreatment_Physical                  374 non-null    float64\n",
      " 27  Plastic pretreatment_Plastic Melted            374 non-null    float64\n",
      " 28  Plastic shape_Fibers                           374 non-null    float64\n",
      " 29  Plastic shape_Pellets                          374 non-null    float64\n",
      " 30  Plastic shape_Shredded                         374 non-null    float64\n",
      " 31  Mixing Process_Dry                             374 non-null    float64\n",
      " 32  Mixing Process_Wet                             374 non-null    float64\n",
      " 33  Aggregates replacement ?_Yes                   374 non-null    float64\n",
      " 34  Bitumen replacement?_Yes                       374 non-null    float64\n",
      "dtypes: float64(35)\n",
      "memory usage: 102.4 KB\n",
      "There is 0 negative values in the new Dataframe\n",
      "C:\\Users\\cavs1\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\impute\\_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n"
     ]
    }
   ],
   "source": [
    "#IMPUTATION OF MISSING VALUES\n",
    "imputer = IterativeImputer (estimator = ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=50)\n",
    "n = imputer.fit_transform(dfMSCleaned)\n",
    "dfMSCleanedImputed = pd.DataFrame(n, columns = list(dfMSCleaned.columns))\n",
    "dfMSCleanedImputed.info()\n",
    "print ('There is '+str(sum(n < 0 for n in dfMSCleanedImputed.values.flatten()))+' negative values in the new Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMSCleanedImputed['New Plastic Type_Nylon'] = dfMSCleanedImputed['New Plastic Type_Nylon'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PE'] = dfMSCleanedImputed['New Plastic Type_PE'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PET'] = dfMSCleanedImputed['New Plastic Type_PET'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PP'] = dfMSCleanedImputed['New Plastic Type_PP'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PU'] = dfMSCleanedImputed['New Plastic Type_PU'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_PVC'] = dfMSCleanedImputed['New Plastic Type_PVC'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_Plastic Mix'] = dfMSCleanedImputed['New Plastic Type_Plastic Mix'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed['New Plastic Type_e-waste'] = dfMSCleanedImputed['New Plastic Type_e-waste'] * dfMSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMSCleanedImputed = dfMSCleanedImputed.drop(['Plastic Addition by bitumen weight (%)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dfMSCleanedImputedScaled = pd.DataFrame(scaler.fit_transform(dfMSCleanedImputed), columns = list(dfMSCleanedImputed.columns))\n",
    "dfMSCleanedImputedScaled.to_clipboard()"
   ]
  },
  {
   "source": [
    "## 2.3 Model Training\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfMSCleanedImputedScaled.loc[:, dfMSCleanedImputedScaled.columns != 'MS of the sample (kN)']\n",
    "y = dfMSCleanedImputedScaled.loc[:,'MS of the sample (kN)']"
   ]
  },
  {
   "source": [
    "### Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'fit_intercept': [True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(LinearRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Lasso Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.001,1, 10, 15, 30, 50, 100],\n",
    "            'fit_intercept':[True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(Lasso(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Ridge Linear regression model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "param_grid = {'alpha': [0,5,15,100],\n",
    "'fit_intercept': [True, False],\n",
    "'solver': [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Linear Elastic net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.01,1,2,3,4],\n",
    "'fit_intercept': [True, False]}\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2],\n",
    "'linearregression__fit_intercept': [True, False],\n",
    "'linearregression__positive':[True, False]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Lasso Polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Lasso(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "            'lasso__alpha': [1, 10, 15, 30, 50, 100],\n",
    "            'lasso__fit_intercept':[True, False],\n",
    "            'lasso__positive': [True, False],\n",
    "            'lasso__max_iter': [3000]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Ridge polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Ridge(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "'ridge__alpha':[10, 20,30,50],\n",
    "'ridge__fit_intercept': [True, False],\n",
    "'ridge__solver': [ 'lsqr', 'cholesky', 'sparse_cg', 'svd', 'sag']}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Support vector regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel':['linear','rbf', 'sigmoid','poly'],\n",
    "    'degree':[2,3],\n",
    "    'C':[0.01,1,5,10],\n",
    "    'epsilon':[0.1,0.5, 1, 1.5]\n",
    "}\n",
    "grid = GridSearchCV(SVR(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Decision Tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth':[2,3,5,10],\n",
    "    'min_samples_split':[2,3,4],\n",
    "    'min_samples_leaf':[1,2]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=['r2','neg_mean_squared_error'], refit='r2', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Extra tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(ExtraTreesRegressor(), param_grid, cv=5, scoring=['r2','neg_mean_squared_error'], refit='r2', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "## Best Model evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "source": [
    "### Ridge polynomial regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree = 2)\n",
    "X_poly = poly_features.fit_transform(X_train)\n",
    "ridgePolyRegre = Ridge(alpha = 10, fit_intercept = True, solver = 'lsqr')\n",
    "ridgePolyRegre.fit(X_poly, y_train)\n",
    "X_poly_test = poly_features.transform(X_test)\n",
    "y_pred = ridgePolyRegre.predict(X_poly_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### Support vector regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportVR = SVR(C = 5, degree = 2, epsilon = 0.1, kernel = 'rbf') \n",
    "SupportVR.fit(X_train, y_train)\n",
    "y_pred = SupportVR.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### Random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndForestR = RandomForestRegressor(n_estimators = 1400, \n",
    "                                    min_samples_split = 5, \n",
    "                                    min_samples_leaf = 2, \n",
    "                                    max_features = 'auto', \n",
    "                                    max_depth = 30, \n",
    "                                    bootstrap = True)\n",
    "rndForestR.fit(X_train, y_train)\n",
    "y_pred = rndForestR.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(x = X.columns, height = rndForestR.feature_importances_)"
   ]
  },
  {
   "source": [
    "### Extra tree Random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraTreeRegressor = ExtraTreesRegressor(n_estimators = 200, \n",
    "                                        min_samples_split = 2,\n",
    "                                        min_samples_leaf = 2,\n",
    "                                        max_features = 'auto',\n",
    "                                        max_depth = None,\n",
    "                                        bootstrap = False)\n",
    "extraTreeRegressor.fit(X_train, y_train)\n",
    "y_pred = extraTreeRegressor.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(x = X.columns, height = extraTreeRegressor.feature_importances_)"
   ]
  },
  {
   "source": [
    "-------------------------------\n",
    "#  3. Marshall Flow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dfMF = eliminateOutliers(dfMF, 'MF of the sample [mm]')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial dataframe size: (316, 34)\nFinal dataframe size: (296, 34)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## 3.1 Data Exploration\n",
    "###  3.1.1 Total Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMF.iloc[:,2:].describe(include = \"all\")\n"
   ]
  },
  {
   "source": [
    "The mean of the MF observations is 3.42, with and stabdard deviation of 0.8. In the Australian standards, the MF usually is between 2 and 5."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfMF[['Aggregate absorption [%]', 'Apparent specific gravity', 'Bitumen content in the sample', 'MF of the sample [mm]']], figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfMF.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap MF', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "source": [
    "The strongest correlation of MF is with the $\\color{red}{\\text{bitumen content}}$ in the sample."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfMF, propertyOfInterest = 'MF of the sample [mm]', columnName1 = \"Agreggate Type\", columnName2 = \"Filler used\", columnName3 = \"Consolidated bitumen penetration grade\", columnName4 = \"Modified asphalt Mix?\")"
   ]
  },
  {
   "source": [
    "The $\\color{red}{\\text{bitumen type}}$ has similar MF, and it happens the same among the samples $\\color{red}{\\text{with and without}}$ plastic modification.\n",
    "###  3.1.2 Modified mixtures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMFModvsUnmod = dfMF [['Modified asphalt Mix?', 'MF of the sample [mm]']]\n",
    "dfMFModvsUnmod.groupby(['Modified asphalt Mix?'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMFModified = dfMF[dfMF['Modified asphalt Mix?'] == 'Yes']\n",
    "dfMFModified.iloc[:,2:].describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInteres = numericColumns[0:2]+numericColumns[10:]+['MF of the sample [mm]']\n",
    "scatter_matrix(dfMFModified[columnsOfInteres], figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfMFModified.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap MF', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "source": [
    "Again, in the modified mixtures, the largest positive correlation is with $\\color{red}{\\text{bitumen content}}$. By looking the graph, one might expect a large correlation with $\\color{red}{\\text{plastic addition}}$, but this is not the case because many observations were skewed to the left."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfMFModified, propertyOfInterest = \"MF of the sample [mm]\", columnName1 = \"Agreggate Type\", columnName2 = \"Plastic shape\", columnName3 = \"New Plastic Type\", columnName4 = \"Mixing Process\")"
   ]
  },
  {
   "source": [
    "No significant difference between dry and wet.\n",
    "###  3.1.3 Wet vs. Dry Mixing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMFWetvsDry = dfMFModified [['Mixing Process', 'MF of the sample [mm]']]\n",
    "dfMFWetvsDry.groupby(['Mixing Process'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfMFModified[columnsOfInteres+['Mixing Process']], hue=\"Mixing Process\", height=2.5)"
   ]
  },
  {
   "source": [
    "##  **MF summary:**\n",
    "\n",
    " *  20 outliers were eliminated from the original sample.\n",
    " *  Total number of observation: 296 -> $\\mu$ = 3.42, $\\sigma$ = 0.8\n",
    " *  $\\color{red}{\\text{Agregate absorption}}$ has a high number of missing values; 40 percent of the data points. Nevertheless, it is still an important value according to the Pearson correlation value (r = -0.15)\n",
    " *  The variable with the largest positive correlation is $\\color{red}{\\text{bitumen content}}$ (r = 0.3). It makes sense as higher quantities of bitumen will increase the cohesion of aggregates in the asphalt mixture.\n",
    " *  Not much difference between the modified and unmodified mixtures -> $\\mu_{modified}$ = 3.44, $\\sigma_{modified}$ = 0.8\n",
    " *  Modified mixtures present positive correlation with $\\color{red}{\\text{bitumen content}}$ (r = 0.33), and negative correlation with mixing properties; $\\color{red}{\\text{shear}}$ (r = -0.24), $\\color{red}{\\text{temperature}}$ (r = -0.2) and $\\color{red}{\\text{mixing time}}$ (r = -0.28).\n",
    " *  Not significan difference between dry and wet. Dry has 180 observations ($\\mu_{dry}$ = 3.49, $\\sigma_{dry}$ = 0.79) while wet has 43 observations ($\\mu_{wet}$ = 3.24, $\\sigma_{wet}$ = 0.81)\n",
    "\n",
    "\n",
    "## 3.2 Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMF.info()"
   ]
  },
  {
   "source": [
    "###  Pre-processing:\n",
    "1.  Eliminate the columns $\\color{red}{\\text{Article ID}}$, $\\color{red}{\\text{Global ID}}$, $\\color{red}{\\text{Aggregate type}}$, $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{filler used}}$, $\\color{red}{\\text{Bitumen type penetration grade}}$, $\\color{red}{\\text{plastic size}}$, $\\color{red}{\\text{Property}}$ and $\\color{red}{\\text{Units}}$.\n",
    "2.  Change the N/a to zero. This is for the unmodified mixtures.\n",
    "3.  Eliminate rows with missing values in $\\color{red}{\\text{New Plastic Type}}$, $\\color{red}{\\text{Plastic addition by bitumen weight}}$ and $\\color{red}{\\text{bitumen content in sample}}$.\n",
    "4.  Change categorical columns to numeric - $\\color{red}{\\text{Modified asphalt Mix?}}$, $\\color{red}{\\text{Consolidated bitumen penetration grade}}$, $\\color{red}{\\text{New Plastic Type}}$, $\\color{red}{\\text{Plastic pretreatment}}$, $\\color{red}{\\text{Plastic shape}}$, $\\color{red}{\\text{Mixing Process}}$, $\\color{red}{\\text{Plastic melted previous to addition?}}$, $\\color{red}{\\text{Replacements}}$.\n",
    "5.  Imputer to $\\color{red}{\\text{Aggregate absorption}}$, $\\color{red}{\\text{gradation}}$, $\\color{red}{\\text{plastic size(mm)}}$, and $\\color{red}{\\text{mixing parameters}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 268 entries, 1 to 316\nData columns (total 36 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption [%]                       151 non-null    float64\n 1   0.075                                          215 non-null    float64\n 2   0.3                                            263 non-null    float64\n 3   0.6                                            232 non-null    float64\n 4   2.36                                           243 non-null    float64\n 5   4.75                                           263 non-null    float64\n 6   9.5                                            232 non-null    float64\n 7   12.5                                           248 non-null    float64\n 8   19                                             263 non-null    float64\n 9   Plastic particle size (mm)                     206 non-null    float64\n 10  Mixing speed (RPM)                             239 non-null    float64\n 11  Mixing Temperature                             252 non-null    float64\n 12  Mixing Time (hours)                            239 non-null    float64\n 13  Plastic Addition by bitumen weight (%)         268 non-null    float64\n 14  Bitumen content in the sample                  268 non-null    float64\n 15  MF of the sample [mm]                          268 non-null    float64\n 16  New Plastic Type_Nylon                         268 non-null    uint8  \n 17  New Plastic Type_PE                            268 non-null    uint8  \n 18  New Plastic Type_PET                           268 non-null    uint8  \n 19  New Plastic Type_PP                            268 non-null    uint8  \n 20  New Plastic Type_PS                            268 non-null    uint8  \n 21  New Plastic Type_PU                            268 non-null    uint8  \n 22  New Plastic Type_PVC                           268 non-null    uint8  \n 23  New Plastic Type_Plastic Mix                   268 non-null    uint8  \n 24  New Plastic Type_e-waste                       268 non-null    uint8  \n 25  Consolidated bitumen penetration grade_50/70   268 non-null    uint8  \n 26  Consolidated bitumen penetration grade_70/100  268 non-null    uint8  \n 27  Plastic pretreatment_Physical                  268 non-null    uint8  \n 28  Plastic pretreatment_Plastic Melted            268 non-null    uint8  \n 29  Plastic shape_Fibers                           268 non-null    uint8  \n 30  Plastic shape_Pellets                          268 non-null    uint8  \n 31  Plastic shape_Shredded                         268 non-null    uint8  \n 32  Mixing Process_Dry                             268 non-null    uint8  \n 33  Mixing Process_Wet                             268 non-null    uint8  \n 34  Aggregates replacement ?_Yes                   268 non-null    uint8  \n 35  Bitumen replacement?_Yes                       268 non-null    uint8  \ndtypes: float64(16), uint8(20)\nmemory usage: 40.8 KB\n"
     ]
    }
   ],
   "source": [
    "#Categorical Variables\n",
    "dfMFCleaned = dfMF.drop(['Article ID', \n",
    "                        'Global ID',\n",
    "                        'Agreggate Type',\n",
    "                        'Modified asphalt Mix?', \n",
    "                        'Apparent specific gravity', \n",
    "                        'Filler used', \n",
    "                        'Bitumen Type Penetration Grade',\n",
    "                        'Plastic Size', \n",
    "                        'Property', \n",
    "                        'Units'], axis = 1)\n",
    "dfMFCleaned = dfMFCleaned.replace('N/a', 0)\n",
    "dfMFCleaned = dfMFCleaned.dropna(subset=['New Plastic Type', \n",
    "                                        'Plastic Addition by bitumen weight (%)', \n",
    "                                        'Bitumen content in the sample'])\n",
    "dfMFCleaned = pd.get_dummies(dfMFCleaned, columns=['New Plastic Type'], drop_first = False)\n",
    "dfMFCleaned = pd.get_dummies(dfMFCleaned, drop_first=True)\n",
    "dfMFCleaned = dfMFCleaned.drop(['New Plastic Type_0'], axis = 1)\n",
    "dfMFCleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268 entries, 0 to 267\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   Aggregate absorption [%]                       268 non-null    float64\n",
      " 1   0.075                                          268 non-null    float64\n",
      " 2   0.3                                            268 non-null    float64\n",
      " 3   0.6                                            268 non-null    float64\n",
      " 4   2.36                                           268 non-null    float64\n",
      " 5   4.75                                           268 non-null    float64\n",
      " 6   9.5                                            268 non-null    float64\n",
      " 7   12.5                                           268 non-null    float64\n",
      " 8   19                                             268 non-null    float64\n",
      " 9   Plastic particle size (mm)                     268 non-null    float64\n",
      " 10  Mixing speed (RPM)                             268 non-null    float64\n",
      " 11  Mixing Temperature                             268 non-null    float64\n",
      " 12  Mixing Time (hours)                            268 non-null    float64\n",
      " 13  Plastic Addition by bitumen weight (%)         268 non-null    float64\n",
      " 14  Bitumen content in the sample                  268 non-null    float64\n",
      " 15  MF of the sample [mm]                          268 non-null    float64\n",
      " 16  New Plastic Type_Nylon                         268 non-null    float64\n",
      " 17  New Plastic Type_PE                            268 non-null    float64\n",
      " 18  New Plastic Type_PET                           268 non-null    float64\n",
      " 19  New Plastic Type_PP                            268 non-null    float64\n",
      " 20  New Plastic Type_PS                            268 non-null    float64\n",
      " 21  New Plastic Type_PU                            268 non-null    float64\n",
      " 22  New Plastic Type_PVC                           268 non-null    float64\n",
      " 23  New Plastic Type_Plastic Mix                   268 non-null    float64\n",
      " 24  New Plastic Type_e-waste                       268 non-null    float64\n",
      " 25  Consolidated bitumen penetration grade_50/70   268 non-null    float64\n",
      " 26  Consolidated bitumen penetration grade_70/100  268 non-null    float64\n",
      " 27  Plastic pretreatment_Physical                  268 non-null    float64\n",
      " 28  Plastic pretreatment_Plastic Melted            268 non-null    float64\n",
      " 29  Plastic shape_Fibers                           268 non-null    float64\n",
      " 30  Plastic shape_Pellets                          268 non-null    float64\n",
      " 31  Plastic shape_Shredded                         268 non-null    float64\n",
      " 32  Mixing Process_Dry                             268 non-null    float64\n",
      " 33  Mixing Process_Wet                             268 non-null    float64\n",
      " 34  Aggregates replacement ?_Yes                   268 non-null    float64\n",
      " 35  Bitumen replacement?_Yes                       268 non-null    float64\n",
      "dtypes: float64(36)\n",
      "memory usage: 75.5 KB\n",
      "There is 0 negative values in the new Dataframe\n",
      "C:\\Users\\cavs1\\Anaconda3\\envs\\spyder\\lib\\site-packages\\sklearn\\impute\\_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n"
     ]
    }
   ],
   "source": [
    "#IMPUTATION OF MISSING VALUES\n",
    "imputer = IterativeImputer (estimator = ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=100)\n",
    "n = imputer.fit_transform(dfMFCleaned)\n",
    "dfMFCleanedImputed = pd.DataFrame(n, columns = list(dfMFCleaned.columns))\n",
    "dfMFCleanedImputed.info()\n",
    "print ('There is '+str(sum(n < 0 for n in dfMFCleanedImputed.values.flatten()))+' negative values in the new Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMFCleanedImputed['New Plastic Type_Nylon'] = dfMFCleanedImputed['New Plastic Type_Nylon'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_PE'] = dfMFCleanedImputed['New Plastic Type_PE'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_PET'] = dfMFCleanedImputed['New Plastic Type_PET'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_PP'] = dfMFCleanedImputed['New Plastic Type_PP'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_PS'] = dfMFCleanedImputed['New Plastic Type_PS'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_PU'] = dfMFCleanedImputed['New Plastic Type_PU'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_PVC'] = dfMFCleanedImputed['New Plastic Type_PVC'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_Plastic Mix'] = dfMFCleanedImputed['New Plastic Type_Plastic Mix'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed['New Plastic Type_e-waste'] = dfMFCleanedImputed['New Plastic Type_e-waste'] * dfMFCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfMFCleanedImputed = dfMFCleanedImputed.drop(['Plastic Addition by bitumen weight (%)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dfMFCleanedImputedScaled = pd.DataFrame(scaler.fit_transform(dfMFCleanedImputed), columns = list(dfMFCleanedImputed.columns))\n",
    "dfMFCleanedImputedScaled.to_clipboard()"
   ]
  },
  {
   "source": [
    "## 3.3 Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "X = dfMFCleanedImputedScaled.loc[:, dfMFCleanedImputedScaled.columns != 'MF of the sample [mm]']\n",
    "y = dfMFCleanedImputedScaled.loc[:,'MF of the sample [mm]']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'fit_intercept': [True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(LinearRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Lasso Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.001,1,2, 10, 15, 30, 50, 100],\n",
    "            'fit_intercept':[True, False],\n",
    "            'positive': [True, False]}\n",
    "grid = GridSearchCV(Lasso(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Ridge Linear Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0,5,15,100],\n",
    "'fit_intercept': [True, False],\n",
    "'solver': [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Linear elastic Net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.01,1,2,3,4],\n",
    "'fit_intercept': [True, False]}\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Ridge(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3],\n",
    "'ridge__alpha':[10, 20,30,50],\n",
    "'ridge__fit_intercept': [True, False],\n",
    "'ridge__solver': [ 'lsqr', 'cholesky', 'sparse_cg', 'svd', 'sag']}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Lasso polynomial model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Lasso(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2,3,4],\n",
    "            'lasso__alpha': [1, 10, 15, 30, 50, 100],\n",
    "            'lasso__fit_intercept':[True, False],\n",
    "            'lasso__positive': [True, False],\n",
    "            'lasso__max_iter': [3000]}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit = 'r2', return_train_score= True)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Ridge Polynomial Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Ridge(**kwargs))\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [3,4],\n",
    "'ridge__alpha':[10, 20,30,50],\n",
    "'ridge__fit_intercept': [True, False],\n",
    "'ridge__solver': [ 'lsqr', 'cholesky', 'sparse_cg', 'svd', 'sag']}\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Support Vector Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel':['linear','rbf', 'sigmoid','poly'],\n",
    "    'degree':[2,3],\n",
    "    'C':[0.01,1,5,10],\n",
    "    'epsilon':[0.1,0.5, 1, 1.5]\n",
    "}\n",
    "grid = GridSearchCV(SVR(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Decision tree regresor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth':[None,2,3,5,10],\n",
    "    'min_samples_split':[2,3,4],\n",
    "    'min_samples_leaf':[1,2,5],\n",
    "    'max_leaf_nodes': [None, 5,10]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=10, scoring=['r2','neg_mean_squared_error'], refit='r2')\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=['r2','neg_mean_squared_error'], refit='r2', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "### Extra tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "grid = RandomizedSearchCV(ExtraTreesRegressor(), param_grid, cv=5, scoring=['r2','neg_mean_squared_error'], refit='r2', n_iter=100)\n",
    "grid.fit(X, y)\n",
    "printBestModel(grid)"
   ]
  },
  {
   "source": [
    "## Best Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "source": [
    "### Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndForestR = RandomForestRegressor(n_estimators = 1200, \n",
    "                                    min_samples_split = 2, \n",
    "                                    min_samples_leaf = 1, \n",
    "                                    max_features = 'auto', \n",
    "                                    max_depth = None, \n",
    "                                    bootstrap = True)\n",
    "rndForestR.fit(X_train, y_train)\n",
    "y_pred = rndForestR.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### Extra tree regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraTreeRegressor = ExtraTreesRegressor(n_estimators = 400, \n",
    "                                        min_samples_split = 2,\n",
    "                                        min_samples_leaf = 1,\n",
    "                                        max_features = 'auto',\n",
    "                                        max_depth = 15,\n",
    "                                        bootstrap = False)\n",
    "extraTreeRegressor.fit(X_train, y_train)\n",
    "y_pred = extraTreeRegressor.predict(X_test)\n",
    "print ('R^2 :'+ str(r2_score(y_test, y_pred)))\n",
    "print ('MAE :' + str(mean_absolute_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = extraTreeRegressor.predict(X_train)\n",
    "r2_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train, y_pred_train)"
   ]
  },
  {
   "source": [
    "-------------------------------\n",
    "#  4. ITS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial dataframe size: (129, 34)\nFinal dataframe size: (121, 34)\n"
     ]
    }
   ],
   "source": [
    "dfITS = eliminateOutliers(dfITS, 'ITS of the sample [Mpa]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITSModified = dfITS[dfITS['Modified asphalt Mix?'] == 'Yes']\n",
    "dfITSModified.iloc[:,2:].describe(include = \"all\")"
   ]
  },
  {
   "source": [
    "## 4.1 Data Exploration\n",
    "###  4.1.1 Total Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITS.describe(include = \"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfITS[['Aggregate absorption [%]', 'Apparent specific gravity', 'Bitumen content in the sample', 'ITS of the sample [Mpa]']], figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfITS.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap ITS', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "source": [
    "The highest positive correlation is with the $\\color{red}{\\text{Apparent specific gravity}}$ (54% missing values). Negative correlation with $\\color{red}{\\text{bitumen content}}$ (this is in disagreement with the Asphalt Mixture selection that states a relation between cracking and bitumen content)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfITS, propertyOfInterest = 'ITS of the sample [Mpa]', columnName1 = \"Agreggate Type\", columnName2 = \"Filler used\", columnName3 = \"Consolidated bitumen penetration grade\", columnName4 = \"Modified asphalt Mix?\")"
   ]
  },
  {
   "source": [
    "No difference between modified and unmodified.\n",
    "\n",
    "###  4.1.2 Modified mixtures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITSModvsUnmod = dfITS [['Modified asphalt Mix?', 'ITS of the sample [Mpa]']]\n",
    "dfITSModvsUnmod.groupby(['Modified asphalt Mix?'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITSModified = dfITS[dfITS['Modified asphalt Mix?'] == 'Yes']\n",
    "dfITSModified.iloc[:,2:].describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInteres = numericColumns[0:2]+numericColumns[10:]+['ITS of the sample [Mpa]']\n",
    "scatter_matrix(dfITSModified[columnsOfInteres], figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfITSModified.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap ITS', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "source": [
    "Some positive correlation in the $\\color{red}{\\text{mixing parameters}}$ and $\\color{red}{\\text{gradation}}$. The ITS property does not behave as a normal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfITSModified, propertyOfInterest = \"ITS of the sample [Mpa]\", columnName1 = \"Agreggate Type\", columnName2 = \"Plastic shape\", columnName3 = \"New Plastic Type\", columnName4 = \"Mixing Process\")"
   ]
  },
  {
   "source": [
    "Possible correlation with $\\color{red}{\\text{plastic shape}}$, but we need to be aware of the sample size. The dry and wet mixing are very similar.\n",
    "###  4.1.3 Wet vs. Dry Mixing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITSWetvsDry = dfITSModified [['Mixing Process', 'ITS of the sample [Mpa]']]\n",
    "dfITSWetvsDry.groupby(['Mixing Process'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfITSModified[columnsOfInteres+['Mixing Process']], hue=\"Mixing Process\", height=2.5)"
   ]
  },
  {
   "source": [
    "##  **ITS summary:**\n",
    "\n",
    " *  8 outliers were eliminated from the original sample.\n",
    " *  Total number of observation: 121 -> $\\mu$ = 0.75, $\\sigma$ = 0.38.\n",
    " *  Major concern with $\\color{red}{\\text{bitumen quantity}}$ as there are 25 missing values.\n",
    " *  $\\color{red}{\\text{Agregate absorption}}$ has a high number of missing values; 54% of the data points. Nevertheless, it is still an important value according to the Pearson correlation value (r = 0.94)\n",
    " *  Negative correlation with $\\color{red}{\\text{Bitumen quantity}}$.\n",
    " *  Not much difference between the modified and unmodified mixtures -> $\\mu_{modified}$ = 0.73,  $\\mu_{unmodified}$ = 0.88. The observations in unmodified are low (16 obs.), so it is not a conclusive statement.\n",
    " *  Modified mixtures present positive correlation with $\\color{red}{\\text{gradation}}$, $\\color{red}{\\text{plastic size}}$ (r = 0.12)and  mixing properties; $\\color{red}{\\text{temperature}}$ (r = 0.19), $\\color{red}{\\text{mixing time}}$ (r = 0.32).\n",
    " *  ITS is not normally distributed.\n",
    " *  Not significan difference between dry and wet. Dry has 69 observations ($\\mu_{dry}$ = 0.74, $\\sigma_{dry}$ = 0.37) while wet has 43 observations ($\\mu_{wet}$ = 0.72, $\\sigma_{wet}$ = 0.39)\n",
    "\n",
    "## 4.2 Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITS.info()"
   ]
  },
  {
   "source": [
    "###  Pre-processing:\n",
    "1.  Eliminate the columns $\\color{red}{\\text{Article ID}}$, $\\color{red}{\\text{Global ID}}$, $\\color{red}{\\text{Aggregate type}}$, $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{filler used}}$, $\\color{red}{\\text{Bitumen type penetration grade}}$, $\\color{red}{\\text{plastic size}}$, $\\color{red}{\\text{Property}}$ and $\\color{red}{\\text{Units}}$.\n",
    "2.  Change the N/a to zero. This is for the unmodified mixtures.\n",
    "4.  Change categorical columns to numeric - $\\color{red}{\\text{Modified asphalt Mix?}}$, $\\color{red}{\\text{Consolidated bitumen penetration grade}}$, $\\color{red}{\\text{New Plastic Type}}$, $\\color{red}{\\text{Plastic pretreatment}}$, $\\color{red}{\\text{Plastic shape}}$, $\\color{red}{\\text{Mixing Process}}$, $\\color{red}{\\text{Plastic melted previous to addition?}}$, $\\color{red}{\\text{Replacements}}$.\n",
    "5.  Imputer to $\\color{red}{\\text{Aggregate absorption}}$, $\\color{red}{\\text{gradations}}$, $\\color{red}{\\text{plastic size(mm)}}$, $\\color{red}{\\text{mixing parameters}}$ and $\\color{red}{\\text{Bitumen content in the sample}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 121 entries, 1 to 129\nData columns (total 32 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption [%]                       95 non-null     float64\n 1   0.075                                          117 non-null    float64\n 2   0.3                                            117 non-null    float64\n 3   0.6                                            121 non-null    float64\n 4   2.36                                           121 non-null    float64\n 5   4.75                                           121 non-null    float64\n 6   9.5                                            121 non-null    float64\n 7   12.5                                           121 non-null    float64\n 8   19                                             121 non-null    float64\n 9   Plastic particle size (mm)                     82 non-null     float64\n 10  Mixing speed (RPM)                             99 non-null     float64\n 11  Mixing Temperature                             111 non-null    float64\n 12  Mixing Time (hours)                            98 non-null     float64\n 13  Plastic Addition by bitumen weight (%)         121 non-null    float64\n 14  Bitumen content in the sample                  96 non-null     float64\n 15  ITS of the sample [Mpa]                        121 non-null    float64\n 16  New Plastic Type_PE                            121 non-null    uint8  \n 17  New Plastic Type_PET                           121 non-null    uint8  \n 18  New Plastic Type_PP                            121 non-null    uint8  \n 19  New Plastic Type_PS                            121 non-null    uint8  \n 20  New Plastic Type_Plastic Mix                   121 non-null    uint8  \n 21  Consolidated bitumen penetration grade_50/70   121 non-null    uint8  \n 22  Consolidated bitumen penetration grade_70/100  121 non-null    uint8  \n 23  Plastic pretreatment_Physical                  121 non-null    uint8  \n 24  Plastic pretreatment_Plastic Melted            121 non-null    uint8  \n 25  Plastic shape_Fibers                           121 non-null    uint8  \n 26  Plastic shape_Pellets                          121 non-null    uint8  \n 27  Plastic shape_Shredded                         121 non-null    uint8  \n 28  Mixing Process_Dry                             121 non-null    uint8  \n 29  Mixing Process_Wet                             121 non-null    uint8  \n 30  Aggregates replacement ?_Yes                   121 non-null    uint8  \n 31  Bitumen replacement?_Yes                       121 non-null    uint8  \ndtypes: float64(16), uint8(16)\nmemory usage: 18.0 KB\n"
     ]
    }
   ],
   "source": [
    "#Categorical Variables\n",
    "dfITSCleaned = dfITS.drop(['Article ID', \n",
    "                        'Global ID',\n",
    "                        'Modified asphalt Mix?',\n",
    "                        'Agreggate Type', \n",
    "                        'Apparent specific gravity', \n",
    "                        'Filler used', \n",
    "                        'Bitumen Type Penetration Grade',\n",
    "                        'Plastic Size', \n",
    "                        'Property', \n",
    "                        'Units'], axis = 1)\n",
    "dfITSCleaned = dfITSCleaned.replace('N/a', 0)\n",
    "dfITSCleaned = pd.get_dummies(dfITSCleaned, columns=['New Plastic Type'], drop_first = False)\n",
    "dfITSCleaned = pd.get_dummies(dfITSCleaned, drop_first=True)\n",
    "dfITSCleaned = dfITSCleaned.drop(['New Plastic Type_0'], axis = 1)\n",
    "dfITSCleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 121 entries, 0 to 120\nData columns (total 32 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption [%]                       121 non-null    float64\n 1   0.075                                          121 non-null    float64\n 2   0.3                                            121 non-null    float64\n 3   0.6                                            121 non-null    float64\n 4   2.36                                           121 non-null    float64\n 5   4.75                                           121 non-null    float64\n 6   9.5                                            121 non-null    float64\n 7   12.5                                           121 non-null    float64\n 8   19                                             121 non-null    float64\n 9   Plastic particle size (mm)                     121 non-null    float64\n 10  Mixing speed (RPM)                             121 non-null    float64\n 11  Mixing Temperature                             121 non-null    float64\n 12  Mixing Time (hours)                            121 non-null    float64\n 13  Plastic Addition by bitumen weight (%)         121 non-null    float64\n 14  Bitumen content in the sample                  121 non-null    float64\n 15  ITS of the sample [Mpa]                        121 non-null    float64\n 16  New Plastic Type_PE                            121 non-null    float64\n 17  New Plastic Type_PET                           121 non-null    float64\n 18  New Plastic Type_PP                            121 non-null    float64\n 19  New Plastic Type_PS                            121 non-null    float64\n 20  New Plastic Type_Plastic Mix                   121 non-null    float64\n 21  Consolidated bitumen penetration grade_50/70   121 non-null    float64\n 22  Consolidated bitumen penetration grade_70/100  121 non-null    float64\n 23  Plastic pretreatment_Physical                  121 non-null    float64\n 24  Plastic pretreatment_Plastic Melted            121 non-null    float64\n 25  Plastic shape_Fibers                           121 non-null    float64\n 26  Plastic shape_Pellets                          121 non-null    float64\n 27  Plastic shape_Shredded                         121 non-null    float64\n 28  Mixing Process_Dry                             121 non-null    float64\n 29  Mixing Process_Wet                             121 non-null    float64\n 30  Aggregates replacement ?_Yes                   121 non-null    float64\n 31  Bitumen replacement?_Yes                       121 non-null    float64\ndtypes: float64(32)\nmemory usage: 30.4 KB\nThere is 0 negative values in the new Dataframe\n"
     ]
    }
   ],
   "source": [
    "#IMPUTATION OF MISSING VALUES\n",
    "imputer = IterativeImputer (estimator = ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=100)\n",
    "n = imputer.fit_transform(dfITSCleaned)\n",
    "dfITSCleanedImputed = pd.DataFrame(n, columns = list(dfITSCleaned.columns))\n",
    "dfITSCleanedImputed.info()\n",
    "print ('There is '+str(sum(n < 0 for n in dfITSCleanedImputed.values.flatten()))+' negative values in the new Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfITSCleanedImputed['New Plastic Type_PE'] = dfITSCleanedImputed['New Plastic Type_PE'] * dfITSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfITSCleanedImputed['New Plastic Type_PET'] = dfITSCleanedImputed['New Plastic Type_PET'] * dfITSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfITSCleanedImputed['New Plastic Type_PP'] = dfITSCleanedImputed['New Plastic Type_PP'] * dfITSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfITSCleanedImputed['New Plastic Type_PS'] = dfITSCleanedImputed['New Plastic Type_PS'] * dfITSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfITSCleanedImputed['New Plastic Type_Plastic Mix'] = dfITSCleanedImputed['New Plastic Type_Plastic Mix'] * dfITSCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfITSCleanedImputed = dfITSCleanedImputed.drop(['Plastic Addition by bitumen weight (%)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dfITSCleanedImputedScaled = pd.DataFrame(scaler.fit_transform(dfITSCleanedImputed), columns = list(dfITSCleanedImputed.columns))\n",
    "dfITSCleanedImputedScaled.to_clipboard()"
   ]
  },
  {
   "source": [
    "-------------------------------\n",
    "#  5. TSR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial dataframe size: (147, 34)\nFinal dataframe size: (146, 34)\n"
     ]
    }
   ],
   "source": [
    "dfTSR = eliminateOutliers(dfTSR, 'TSR of the sample[%]')"
   ]
  },
  {
   "source": [
    "## 5.1 Data Exploration\n",
    "###  5.1.1 Total Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTSR.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dfTSR[['Aggregate absorption [%]', 'Apparent specific gravity', 'Bitumen content in the sample', 'TSR of the sample[%]']], figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfTSR.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap TSR', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "source": [
    "Positive correlation with $\\color{red}{\\text{gradiation}}$ and negative with $\\color{red}{\\text{Aggregate absorption}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfTSR, propertyOfInterest = 'TSR of the sample[%]', columnName1 = \"Agreggate Type\", columnName2 = \"Filler used\", columnName3 = \"Consolidated bitumen penetration grade\", columnName4 = \"Modified asphalt Mix?\")"
   ]
  },
  {
   "source": [
    "It exists an strong difference between modified and unmodified asphalt mixture.\n",
    "\n",
    "###  5.1.2 Modified mixtures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTSRModvsUnmod = dfTSR [['Modified asphalt Mix?', 'TSR of the sample[%]']]\n",
    "dfTSRModvsUnmod.groupby(['Modified asphalt Mix?'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTSRModified = dfTSR[dfTSR['Modified asphalt Mix?'] == 'Yes']\n",
    "dfTSRModified.iloc[:,2:].describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInteres = numericColumns[0:2]+numericColumns[10:]+['TSR of the sample[%]']\n",
    "scatter_matrix(dfTSRModified[columnsOfInteres], figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(dfTSRModified.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap TSR', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBoxPlotGraphs(dataFrame = dfTSRModified, propertyOfInterest = \"TSR of the sample[%]\", columnName1 = \"Agreggate Type\", columnName2 = \"Plastic shape\", columnName3 = \"New Plastic Type\", columnName4 = \"Mixing Process\")"
   ]
  },
  {
   "source": [
    "The large correlation is with $\\color{red}{\\text{plastic addition quantity}}$, and it exists a significant difference between $\\color{red}{\\text{wet}}$ and $\\color{red}{\\text{dry}}$ mixing.\n",
    "\n",
    "### 5.1.3 Wet vs. Dry Mixing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTSRWetvsDry = dfTSRModified [['Mixing Process', 'TSR of the sample[%]']]\n",
    "dfTSRWetvsDry.groupby(['Mixing Process'], as_index=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfTSRModified[columnsOfInteres+['Mixing Process']], hue=\"Mixing Process\", height=2.5)"
   ]
  },
  {
   "source": [
    "##  **TSR summary:**\n",
    "\n",
    " *  1 outliers were eliminated from the original sample.\n",
    " *  Total number of observation: 146 -> $\\mu$ = 84, $\\sigma$ = 10.31.\n",
    " *  Highest positive correlation with $\\color{red}{\\text{gradation}}$, and it exists a negative correlation with $\\color{red}{\\text{aggregate absorptionn}}$ (r = -0.18)\n",
    " *  Some difference between the modified and unmodified mixtures -> $\\mu_{modified}$ = 86.48,  $\\mu_{unmodified}$ = 77.65.\n",
    " *  Modified mixtures present positive correlation with $\\color{red}{\\text{plastic addition}}$ (r = 0.26).\n",
    " *  Possible difference between dry and wet.  Dry ($\\mu_{dry}$ = 88, $\\sigma_{dry}$ = 8.9) vs. wet ($\\mu_{wet}$ = 83.7, $\\sigma_{wet}$ = 8.3) -> This is not conclusive because the variance of both sample groups is large.\n",
    "\n",
    "## 5.2 Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTSR.info()"
   ]
  },
  {
   "source": [
    "###  Pre-processing:\n",
    "1.  Eliminate the columns $\\color{red}{\\text{Article ID}}$, $\\color{red}{\\text{Global ID}}$, $\\color{red}{\\text{Aggregate type}}$, $\\color{red}{\\text{Apparent specific gravity}}$, $\\color{red}{\\text{filler used}}$, $\\color{red}{\\text{Bitumen type penetration grade}}$, $\\color{red}{\\text{plastic size}}$, $\\color{red}{\\text{Property}}$ and $\\color{red}{\\text{Units}}$.\n",
    "2.  Change the N/a to zero. This is for the unmodified mixtures.\n",
    "4.  Change categorical columns to numeric - $\\color{red}{\\text{Modified asphalt Mix?}}$, $\\color{red}{\\text{Consolidated bitumen penetration grade}}$, $\\color{red}{\\text{New Plastic Type}}$, $\\color{red}{\\text{Plastic pretreatment}}$, $\\color{red}{\\text{Plastic shape}}$, $\\color{red}{\\text{Mixing Process}}$, $\\color{red}{\\text{Plastic melted previous to addition?}}$, $\\color{red}{\\text{Replacements}}$.\n",
    "5.  Imputer to $\\color{red}{\\text{Aggregate absorption}}$, $\\color{red}{\\text{gradations}}$, $\\color{red}{\\text{plastic size(mm)}}$, $\\color{red}{\\text{mixing parameters}}$ and $\\color{red}{\\text{Bitumen content in the sample}}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 146 entries, 1 to 147\nData columns (total 32 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption [%]                       108 non-null    float64\n 1   0.075                                          139 non-null    float64\n 2   0.3                                            139 non-null    float64\n 3   0.6                                            143 non-null    float64\n 4   2.36                                           143 non-null    float64\n 5   4.75                                           143 non-null    float64\n 6   9.5                                            143 non-null    float64\n 7   12.5                                           143 non-null    float64\n 8   19                                             143 non-null    float64\n 9   Plastic particle size (mm)                     113 non-null    float64\n 10  Mixing speed (RPM)                             134 non-null    float64\n 11  Mixing Temperature                             146 non-null    float64\n 12  Mixing Time (hours)                            133 non-null    float64\n 13  Plastic Addition by bitumen weight (%)         146 non-null    float64\n 14  Bitumen content in the sample                  121 non-null    float64\n 15  TSR of the sample[%]                           146 non-null    float64\n 16  New Plastic Type_PE                            146 non-null    uint8  \n 17  New Plastic Type_PET                           146 non-null    uint8  \n 18  New Plastic Type_PP                            146 non-null    uint8  \n 19  New Plastic Type_PS                            146 non-null    uint8  \n 20  New Plastic Type_Plastic Mix                   146 non-null    uint8  \n 21  Consolidated bitumen penetration grade_50/70   146 non-null    uint8  \n 22  Consolidated bitumen penetration grade_70/100  146 non-null    uint8  \n 23  Plastic pretreatment_Physical                  146 non-null    uint8  \n 24  Plastic pretreatment_Plastic Melted            146 non-null    uint8  \n 25  Plastic shape_Fibers                           146 non-null    uint8  \n 26  Plastic shape_Pellets                          146 non-null    uint8  \n 27  Plastic shape_Shredded                         146 non-null    uint8  \n 28  Mixing Process_Dry                             146 non-null    uint8  \n 29  Mixing Process_Wet                             146 non-null    uint8  \n 30  Aggregates replacement ?_Yes                   146 non-null    uint8  \n 31  Bitumen replacement?_Yes                       146 non-null    uint8  \ndtypes: float64(16), uint8(16)\nmemory usage: 21.7 KB\n"
     ]
    }
   ],
   "source": [
    "dfTSRCleaned = dfTSR.drop(['Article ID', \n",
    "                        'Global ID',\n",
    "                        'Modified asphalt Mix?',\n",
    "                        'Agreggate Type', \n",
    "                        'Apparent specific gravity', \n",
    "                        'Filler used', \n",
    "                        'Bitumen Type Penetration Grade',\n",
    "                        'Plastic Size', \n",
    "                        'Property', \n",
    "                        'Units'], axis = 1)\n",
    "dfTSRCleaned = dfTSRCleaned.replace('N/a', 0)\n",
    "dfTSRCleaned = pd.get_dummies(dfTSRCleaned, columns=['New Plastic Type'], drop_first = False)\n",
    "dfTSRCleaned = pd.get_dummies(dfTSRCleaned, drop_first=True)\n",
    "dfTSRCleaned = dfTSRCleaned.drop(['New Plastic Type_0'], axis = 1)\n",
    "dfTSRCleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 146 entries, 0 to 145\nData columns (total 32 columns):\n #   Column                                         Non-Null Count  Dtype  \n---  ------                                         --------------  -----  \n 0   Aggregate absorption [%]                       146 non-null    float64\n 1   0.075                                          146 non-null    float64\n 2   0.3                                            146 non-null    float64\n 3   0.6                                            146 non-null    float64\n 4   2.36                                           146 non-null    float64\n 5   4.75                                           146 non-null    float64\n 6   9.5                                            146 non-null    float64\n 7   12.5                                           146 non-null    float64\n 8   19                                             146 non-null    float64\n 9   Plastic particle size (mm)                     146 non-null    float64\n 10  Mixing speed (RPM)                             146 non-null    float64\n 11  Mixing Temperature                             146 non-null    float64\n 12  Mixing Time (hours)                            146 non-null    float64\n 13  Plastic Addition by bitumen weight (%)         146 non-null    float64\n 14  Bitumen content in the sample                  146 non-null    float64\n 15  TSR of the sample[%]                           146 non-null    float64\n 16  New Plastic Type_PE                            146 non-null    float64\n 17  New Plastic Type_PET                           146 non-null    float64\n 18  New Plastic Type_PP                            146 non-null    float64\n 19  New Plastic Type_PS                            146 non-null    float64\n 20  New Plastic Type_Plastic Mix                   146 non-null    float64\n 21  Consolidated bitumen penetration grade_50/70   146 non-null    float64\n 22  Consolidated bitumen penetration grade_70/100  146 non-null    float64\n 23  Plastic pretreatment_Physical                  146 non-null    float64\n 24  Plastic pretreatment_Plastic Melted            146 non-null    float64\n 25  Plastic shape_Fibers                           146 non-null    float64\n 26  Plastic shape_Pellets                          146 non-null    float64\n 27  Plastic shape_Shredded                         146 non-null    float64\n 28  Mixing Process_Dry                             146 non-null    float64\n 29  Mixing Process_Wet                             146 non-null    float64\n 30  Aggregates replacement ?_Yes                   146 non-null    float64\n 31  Bitumen replacement?_Yes                       146 non-null    float64\ndtypes: float64(32)\nmemory usage: 36.6 KB\nThere is 0 negative values in the new Dataframe\n"
     ]
    }
   ],
   "source": [
    "#IMPUTATION OF MISSING VALUES\n",
    "imputer = IterativeImputer (estimator = ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=100)\n",
    "n = imputer.fit_transform(dfTSRCleaned)\n",
    "dfTSRCleanedImputed = pd.DataFrame(n, columns = list(dfTSRCleaned.columns))\n",
    "dfTSRCleanedImputed.info()\n",
    "print ('There is '+str(sum(n < 0 for n in dfTSRCleanedImputed.values.flatten()))+' negative values in the new Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTSRCleanedImputed['New Plastic Type_PE'] = dfTSRCleanedImputed['New Plastic Type_PE'] * dfTSRCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfTSRCleanedImputed['New Plastic Type_PET'] = dfTSRCleanedImputed['New Plastic Type_PET'] * dfTSRCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfTSRCleanedImputed['New Plastic Type_PP'] = dfTSRCleanedImputed['New Plastic Type_PP'] * dfTSRCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfTSRCleanedImputed['New Plastic Type_PS'] = dfTSRCleanedImputed['New Plastic Type_PS'] * dfTSRCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfTSRCleanedImputed['New Plastic Type_Plastic Mix'] = dfTSRCleanedImputed['New Plastic Type_Plastic Mix'] * dfTSRCleanedImputed['Plastic Addition by bitumen weight (%)']\n",
    "dfTSRCleanedImputed = dfTSRCleanedImputed.drop(['Plastic Addition by bitumen weight (%)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dfTSRCleanedImputedScaled = pd.DataFrame(scaler.fit_transform(dfTSRCleanedImputed), columns = list(dfTSRCleanedImputed.columns))\n",
    "dfTSRCleanedImputedScaled.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}